# AI ë¶ˆëŸ‰ë¥  ì¸¡ì •ê¸°

## ì‹œë‚˜ë¦¬ì˜¤ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.
1. ë„ì¥ ê³µì • ìš´ì˜
- ì „êµ­ì˜ ì¤‘ì†Œ ì œì¡° ê¸°ì—…ë“¤ì´ í˜„ì¥ í™˜ê²½ ë³€í™”ì— ëŠ¥ë™ì ìœ¼ë¡œ ëŒ€ì‘í•  ìˆ˜ ìˆë„ë¡ ì„¼ì„œ ê¸°ë°˜ ê³µì • ë°ì´í„° ìˆ˜ì§‘ ë° ì‹œê°í™” ì‹œìŠ¤í…œ ì„¤ê³„

2. ì‹œìŠ¤í…œ íŠ¹ì§•
- ì´ ì‹œìŠ¤í…œì€ ê¸°ì—…ì˜ ì„¤ë¹„ ê·œëª¨ë‚˜ ì¸í”„ë¼ì— ê´€ê³„ì—†ì´ ì ìš© ê°€ëŠ¥í•˜ë„ë¡ ëª¨ë“ˆí™”
- ì‹¤ì‹œê°„ ê³µì • ëª¨ë‹ˆí„°ë§

ìµœì¢… ëª©í‘œ: ì™¸ë¶€ í™˜ê²½ ì¡°ê±´ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ê°ì§€ ë° ê¸°ë¡
           í˜„ì¥ ì‘ì—…ìê°€ ê³µì • ìƒíƒœë¥¼ ì§ê´€ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆë„ë¡ ì‹œê°í™”

## ì—­í• 
ì´ì¬ì°¬(íŒ€ì¥)
ê°€ìƒ ê³µì • êµ¬í˜„, ë¡œë´‡ ë°ì´í„° ê´€ë¦¬ //
ê¹€ë¯¼ì¬(Me)
ì´ë¯¸ì§€ ë°ì´í„° ê´€ë¦¬(with AI) //
ë°•ì¤€ìš°
ì´ë¯¸ì§€ ë°ì´í„° ê²Œë”ë§ ë° ì „ì†¡ //
ë°•ë¯¼ì¤€
ì˜¨/ìŠµë„ ì„¼ì„œ ê´€ë¦¬ / dash ê°€ì‹œí™” //
ì´ì¬ë¯¼
ë‰´ë©”ë¦­ ë°ì´í„° ê°€ì‹œí™” / dash ê°€ì‹œí™” 

## ê³µì • í”„ë¡œì„¸ìŠ¤ ë° ê²°ê³¼ë¬¼
<img width="1050" height="619" alt="Image" src="https://github.com/user-attachments/assets/a8eadb49-1dfb-41df-8645-ac6d7085f396" />

<img width="1662" height="447" alt="Image" src="https://github.com/user-attachments/assets/cda4867b-6566-4e7b-9ae7-6d9028241946" />


<img width="1745" height="1265" alt="Image" src="https://github.com/user-attachments/assets/c37f452c-df91-425e-a9eb-96529d3ee417" />

### ì´ë¯¸ì§€ ì½”ë“œ, tab5 5ê°œë¥¼ ë¶„í• í•˜ì§€ ì•Šê³  í•©ì³ë†“ì•˜ë‹¤. 
> main ê¸°ëŠ¥: ì „ì—­ ë³€ìˆ˜ë¡œ API í˜¸ì¶œ, gpt ëª¨ë¸ì„ ì´ìš©í•´ ë¶ˆëŸ‰ë¥  ê²€ì¶œ(í”„ë¡¬í”„íŠ¸ë¥¼ í†µí•´ ì‹œë‚˜ë¦¬ì˜¤ ê°€ì •í•œ ê±¸ ì ìš©ì‹œí‚´)
> 
> íŒŒì¸íŠœë‹ í”„ë¡œì„¸ìŠ¤ë¥¼ í†µí•´ ì‚¬ìš©ìê°€ AIì˜ ë¶ˆëŸ‰ë¥  íŒë‹¨ì„ ì¬ê²€í† í•˜ì—¬ ì„±ëŠ¥ í–¥ìƒ ê°€ëŠ¥


```py

import streamlit as st
from dotenv import load_dotenv
import os
from langchain_teddynote import logging
from langchain_openai import ChatOpenAI
from langchain_teddynote.models import MultiModal
import socket
from datetime import datetime
from datetime import timedelta
import tempfile
import shutil
import requests
import json
import time
import threading
from flask import Flask, request, jsonify, send_from_directory, make_response
from werkzeug.utils import secure_filename
import io
from PIL import Image
import base64
import queue
import glob
import pandas as pd
import csv
import plotly.express as px
import plotly.graph_objects as go
from collections import Counter
from openai import OpenAI
import zipfile
import sys
from flask_cors import CORS
import openai
from typing import List, Dict, Any


# API key ì •ë³´ ë¡œë“œ
load_dotenv()

logging.langsmith("project ì´ë¯¸ì§€ ì¸ì‹")

# ì „ì—­ ë³€ìˆ˜ ë° ì„¤ì •
UPLOAD_FOLDER = "received_images"
DATA_LOG_FOLDER = "analysis_logs"
FINETUNE_FOLDER = "finetuning_data"
FINETUNE_JOBS_FOLDER = "finetuning_jobs"
ALLOWED_EXTENSIONS = {"png", "jpg", "jpeg", "gif"}

# ì•± ì´ë¯¸ì§€ ì—…ë¡œë“œ
image_queue = queue.Queue()
flask_app = Flask(__name__)
CORS(flask_app, resources={r"/files/*": {"origins": "*"}})
flask_app.config["UPLOAD_FOLDER"] = UPLOAD_FOLDER

# í´ë” ìƒì„±
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(DATA_LOG_FOLDER, exist_ok=True)
os.makedirs(FINETUNE_FOLDER, exist_ok=True)
os.makedirs(FINETUNE_JOBS_FOLDER, exist_ok=True)

# CSV ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
LOG_CSV_PATH = os.path.join(DATA_LOG_FOLDER, "analysis_logs.csv")  # ê°€ì‹œí™” ë¨
TRAINING_DATA_PATH = os.path.join(FINETUNE_FOLDER, "training_data.jsonl")
VALIDATION_DATA_PATH = os.path.join(FINETUNE_FOLDER, "validation_data.jsonl")
FINETUNE_JOBS_PATH = os.path.join(FINETUNE_JOBS_FOLDER, "finetune_jobs.json")

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))


def initialize_csv_log():
    """CSV ë¡œê·¸ íŒŒì¼ ì´ˆê¸°í™”"""
    if not os.path.exists(LOG_CSV_PATH):
        headers = [
            "timestamp",
            "date",
            "time",
            "filename",
            "original_name",
            "file_size_mb",
            "image_width",
            "image_height",
            "model_used",
            "analysis_result",
            "judgment",
            "confidence_score",
            "processing_time_seconds",
            "defect_type",
            "notes",
            "user_feedback",
            "feedback_timestamp",
            "used_for_training",
        ]

        with open(LOG_CSV_PATH, "w", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(headers)


def initialize_finetune_jobs():
    """íŒŒì¸íŠœë‹ ì‘ì—… ë¡œê·¸ ì´ˆê¸°í™”"""
    if not os.path.exists(FINETUNE_JOBS_PATH):
        with open(FINETUNE_JOBS_PATH, "w", encoding="utf-8") as f:
            json.dump([], f, ensure_ascii=False, indent=2)


def load_finetune_jobs():
    try:
        with open("finetune_jobs.json", "r", encoding="utf-8") as f:
            return json.load(f)  # ë¦¬ìŠ¤íŠ¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
    except Exception:
        return []


def save_finetune_jobs(jobs: list):
    """íŒŒì¸íŠœë‹ ì‘ì—… ë¦¬ìŠ¤íŠ¸ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
    try:
        with open(FINETUNE_JOBS_PATH, "w", encoding="utf-8") as f:
            json.dump(jobs, f, ensure_ascii=False, indent=2)
        print(f"âœ… ì‘ì—… ì •ë³´ê°€ {FINETUNE_JOBS_PATH}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì‘ì—… ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


def add_finetune_job(job_data: Dict[str, Any]) -> bool:
    try:
        jobs = load_finetune_jobs()
        job_ids = {j.get("job_id") for j in jobs if "job_id" in j}
        if job_data.get("job_id") in job_ids:
            print(f"ğŸ” ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ì‘ì—…: {job_data['job_id']} ì €ì¥ ì•ˆ í•¨")
            return False
        jobs.append(job_data)
        saved = save_finetune_jobs(jobs)
        if saved:
            print(f"âœ… ìƒˆ ì‘ì—… ì €ì¥ ì™„ë£Œ: {job_data['job_id']}")
        return saved
    except Exception as e:
        print(f"âŒ ì‘ì—… ì¶”ê°€ ì¤‘ ì˜¤ë¥˜: {e}")
        return False


def extract_judgment_info(analysis_text):
    """ë¶„ì„ ê²°ê³¼ì—ì„œ íŒì • ì •ë³´ ì¶”ì¶œ"""
    analysis_lower = analysis_text.lower()

    # íŒì • ê²°ê³¼ ì¶”ì¶œ
    if "ë¶ˆëŸ‰" in analysis_lower:
        judgment = "ë¶ˆëŸ‰"
    elif "ì •í’ˆ" in analysis_lower or "ì •ìƒ" in analysis_lower:
        judgment = "ì •í’ˆ"
    else:
        judgment = "ë¯¸ë¶„ë¥˜"

    # ì‹ ë¢°ë„ ì¶”ì¶œ (% ê¸°í˜¸ê°€ ìˆëŠ” ìˆ«ì ì°¾ê¸°)
    import re

    confidence_match = re.search(r"(\d+(?:\.\d+)?)\s*%", analysis_text)
    confidence_score = float(confidence_match.group(1)) if confidence_match else None

    # ë¶ˆëŸ‰ ìœ í˜• ì¶”ì¶œ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜)
    defect_keywords = {
        "ìƒ‰ìƒ": ["ìƒ‰ìƒ", "ì»¬ëŸ¬", "ë¹¨ê°„ìƒ‰", "íŒŒë€ìƒ‰"],
        "í˜•íƒœ": ["í˜•íƒœ", "ëª¨ì–‘", "ë³€í˜•"],
        "í‘œë©´": ["í‘œë©´", "ìŠ¤í¬ë˜ì¹˜", "ê¸í˜"],
        "í¬ê¸°": ["í¬ê¸°", "ì‚¬ì´ì¦ˆ", "ì¹˜ìˆ˜"],
    }

    defect_type = "ê¸°íƒ€"
    for defect, keywords in defect_keywords.items():
        if any(keyword in analysis_lower for keyword in keywords):
            defect_type = defect
            break

    return judgment, confidence_score, defect_type


def get_image_info(image_path):
    """ì´ë¯¸ì§€ íŒŒì¼ ì •ë³´ ì¶”ì¶œ"""
    try:
        # íŒŒì¼ í¬ê¸° (MB)
        file_size_mb = round(os.path.getsize(image_path) / (1024 * 1024), 2)

        # ì´ë¯¸ì§€ í¬ê¸°
        with Image.open(image_path) as img:
            width, height = img.size

        return file_size_mb, width, height
    except Exception as e:
        st.error(f"ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return 0, 0, 0


def log_analysis_result(
    image_path, original_name, analysis_result, model_used, processing_time
):
    """ë¶„ì„ ê²°ê³¼ë¥¼ CSVì— ê¸°ë¡"""
    try:
        now = datetime.now()

        # ì´ë¯¸ì§€ ì •ë³´ ì¶”ì¶œ
        file_size_mb, width, height = get_image_info(image_path)

        # íŒì • ì •ë³´ ì¶”ì¶œ
        judgment, confidence_score, defect_type = extract_judgment_info(analysis_result)

        # ë¡œê·¸ ë°ì´í„° ì¤€ë¹„
        log_data = [
            now.isoformat(),  # timestamp
            now.strftime("%Y-%m-%d"),  # date
            now.strftime("%H:%M:%S"),  # time
            os.path.basename(image_path),  # filename
            original_name,  # original_name
            file_size_mb,  # file_size_mb
            width,  # image_width
            height,  # image_height
            model_used,  # model_used
            analysis_result.replace("\n", " "),  # analysis_result (ê°œí–‰ ì œê±°)
            judgment,  # judgment
            confidence_score,  # confidence_score
            round(processing_time, 2),  # processing_time_seconds
            defect_type,  # defect_type
            "",  # notes (ì¶”í›„ ìˆ˜ë™ ì…ë ¥ ê°€ëŠ¥)
            "",  # user_feedback
            "",  # feedback_timestamp
            "No",  # used_for_training
        ]

        # CSVì— ì¶”ê°€
        with open(LOG_CSV_PATH, "a", newline="", encoding="utf-8") as f:
            writer = csv.writer(f)
            writer.writerow(log_data)

        return True
    except Exception as e:
        st.error(f"ë¡œê·¸ ê¸°ë¡ ì˜¤ë¥˜: {e}")
        return False


if "finetune_jobs_json" not in st.session_state:
    st.session_state.finetune_jobs_json = load_finetune_jobs()


def update_user_feedback(index, feedback, correct_judgment):
    """ì‚¬ìš©ì í”¼ë“œë°± ì—…ë°ì´íŠ¸"""
    try:
        df = pd.read_csv(LOG_CSV_PATH, encoding="utf-8")

        if index < len(df):
            df.loc[index, "user_feedback"] = correct_judgment
            df.loc[index, "feedback_timestamp"] = datetime.now().isoformat()
            df.loc[index, "notes"] = feedback

            df.to_csv(LOG_CSV_PATH, index=False)
            return True
        return False
    except Exception as e:
        st.error(f"í”¼ë“œë°± ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
        return False


def prepare_training_data():
    """í›ˆë ¨ ë°ì´í„° ì¤€ë¹„"""
    try:
        df = pd.read_csv(LOG_CSV_PATH, encoding="utf-8")

        # í”¼ë“œë°±ì´ ìˆëŠ” ë°ì´í„°ë§Œ í•„í„°ë§
        feedback_data = df[df["user_feedback"].notna() & (df["user_feedback"] != "")]

        if len(feedback_data) == 0:
            return 0, "í”¼ë“œë°± ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        training_samples = []

        for _, row in feedback_data.iterrows():
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€
            system_message = {
                "role": "system",
                "content": st.session_state.get(
                    "system_prompt",
                    "ë‹¹ì‹ ì€ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ë„ì¥ ë¶ˆëŸ‰ë¥ ì„ íŒë‹¨í•˜ëŠ” ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
                ),
            }

            # ì‚¬ìš©ì ë©”ì‹œì§€ (ì´ë¯¸ì§€ ë¶„ì„ ìš”ì²­)
            user_message = {
                "role": "user",
                "content": "ì´ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ë¶ˆëŸ‰/ì •í’ˆì„ íŒë‹¨í•´ì£¼ì„¸ìš”.",
            }

            # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ (ì˜¬ë°”ë¥¸ ë‹µë³€)
            correct_judgment = row["user_feedback"]
            assistant_content = f"ë¶„ì„ ê²°ê³¼: {correct_judgment}"

            if correct_judgment == "ë¶ˆëŸ‰":
                assistant_content += "\nì´ ì œí’ˆì€ ë¶ˆëŸ‰í’ˆìœ¼ë¡œ íŒì •ë©ë‹ˆë‹¤."
            elif correct_judgment == "ì •í’ˆ":
                assistant_content += "\nì´ ì œí’ˆì€ ì •í’ˆìœ¼ë¡œ íŒì •ë©ë‹ˆë‹¤."

            assistant_message = {"role": "assistant", "content": assistant_content}

            # í›ˆë ¨ ìƒ˜í”Œ ìƒì„±
            training_sample = {
                "messages": [system_message, user_message, assistant_message]
            }

            training_samples.append(training_sample)

        # í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¶„í•  (80:20)
        split_idx = int(len(training_samples) * 0.8)
        training_data = training_samples[:split_idx]
        validation_data = training_samples[split_idx:]

        # JSONL íŒŒì¼ë¡œ ì €ì¥
        with open(TRAINING_DATA_PATH, "w", encoding="utf-8") as f:
            for sample in training_data:
                f.write(json.dumps(sample, ensure_ascii=False) + "\n")

        if validation_data:
            with open(VALIDATION_DATA_PATH, "w", encoding="utf-8") as f:
                for sample in validation_data:
                    f.write(json.dumps(sample, ensure_ascii=False) + "\n")

        # ì‚¬ìš©ëœ ë°ì´í„° ë§ˆí‚¹
        used_indices = feedback_data.index.tolist()
        df.loc[used_indices, "used_for_training"] = "Yes"
        df.to_csv(LOG_CSV_PATH, index=False)

        return (
            len(training_samples),
            f"í›ˆë ¨ ë°ì´í„° {len(training_data)}ê°œ, ê²€ì¦ ë°ì´í„° {len(validation_data)}ê°œ ì¤€ë¹„ ì™„ë£Œ",
        )

    except Exception as e:
        return 0, f"í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì˜¤ë¥˜: {e}"


def cancel_job(job_id: str):
    """íŒŒì¸íŠœë‹ ì‘ì—…ì„ ì·¨ì†Œí•©ë‹ˆë‹¤."""
    try:
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
        api_key = os.getenv("OPENAI_API_KEY")

        if not api_key:
            raise Exception(
                "âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            )

        # OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = openai.OpenAI(api_key=api_key)

        print("ì‘ì—…ì„ ì·¨ì†Œí•˜ëŠ” ì¤‘...")

        # íŒŒì¸íŠœë‹ ì‘ì—… ì·¨ì†Œ
        response = client.fine_tuning.jobs.cancel(job_id)

        if response.status == "cancelled":
            print("âœ… ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")

            # ë¡œì»¬ ëª©ë¡ ì—…ë°ì´íŠ¸ (load_finetune_jobs, save_finetune_jobs í•¨ìˆ˜ê°€ ìˆë‹¤ê³  ê°€ì •)
            try:
                jobs = load_finetune_jobs()
                for job in jobs:
                    if job.get("job_id") == job_id:
                        job["status"] = "cancelled"
                        job["updated_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        break
                save_finetune_jobs(jobs)
                print("ë¡œì»¬ ì‘ì—… ëª©ë¡ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as local_update_error:
                print(f"âš ï¸ ë¡œì»¬ ëª©ë¡ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {local_update_error}")

            return True
        else:
            print(f"ì‘ì—… ìƒíƒœ: {response.status}")
            return False

    except openai.APIError as e:
        if "not found" in str(e).lower():
            raise Exception(f"ì‘ì—… ID '{job_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        elif "insufficient_quota" in str(e):
            raise Exception("API ì‚¬ìš©ëŸ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ê²°ì œ ì •ë³´ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            raise Exception(f"OpenAI API ì˜¤ë¥˜: {str(e)}")
    except Exception as e:
        raise Exception(f"ì‘ì—… ì·¨ì†Œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def check_job_status(job_id: str):
    """íŠ¹ì • ì‘ì—…ì˜ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
        api_key = os.getenv("OPENAI_API_KEY")

        if not api_key:
            raise Exception(
                "âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            )

        # OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = openai.OpenAI(api_key=api_key)

        print("ì‘ì—… ìƒíƒœë¥¼ í™•ì¸í•˜ëŠ” ì¤‘...")

        # íŒŒì¸íŠœë‹ ì‘ì—… ìƒíƒœ ì¡°íšŒ
        response = client.fine_tuning.jobs.retrieve(job_id)

        # ìƒíƒœ ì •ë³´ í‘œì‹œ
        status_emoji = {
            "running": "ğŸŸ¡",
            "succeeded": "ğŸŸ¢",
            "failed": "ğŸ”´",
            "cancelled": "âš«",
        }.get(response.status, "âšª")

        print(f"{status_emoji} í˜„ì¬ ìƒíƒœ: {response.status}")

        # ìƒíƒœë³„ ì¶”ê°€ ì •ë³´ í‘œì‹œ
        if response.status == "succeeded":
            print(f"ğŸ‰ íŒŒì¸íŠœë‹ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"ëª¨ë¸: {response.fine_tuned_model}")
        elif response.status == "failed":
            error_msg = response.error.message if response.error else "Unknown error"
            print(f"âŒ íŒŒì¸íŠœë‹ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {error_msg}")
        elif response.status == "running":
            print("â³ íŒŒì¸íŠœë‹ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤...")
        elif response.status == "cancelled":
            print("âš« ì‘ì—…ì´ ì·¨ì†Œë˜ì—ˆìŠµë‹ˆë‹¤.")

        # ê¸°ë³¸ ì‘ì—… ì •ë³´ ì¶œë ¥
        print(f"\nì‘ì—… ì„¸ë¶€ ì •ë³´:")
        print(f"- ì‘ì—… ID: {response.id}")
        print(f"- ëª¨ë¸: {response.model}")
        print(
            f"- ìƒì„± ì‹œê°„: {datetime.fromtimestamp(response.created_at) if response.created_at else 'N/A'}"
        )

        if response.finished_at:
            print(f"- ì™„ë£Œ ì‹œê°„: {datetime.fromtimestamp(response.finished_at)}")

        if response.trained_tokens:
            print(f"- í›ˆë ¨ëœ í† í° ìˆ˜: {response.trained_tokens:,}")

        # ë¡œì»¬ ëª©ë¡ ì—…ë°ì´íŠ¸
        try:
            jobs = load_finetune_jobs()
            for job in jobs:
                if job.get("job_id") == job_id:
                    job["status"] = response.status
                    job["finished_at"] = (
                        datetime.fromtimestamp(response.finished_at).strftime(
                            "%Y-%m-%d %H:%M:%S"
                        )
                        if response.finished_at
                        else None
                    )
                    job["fine_tuned_model"] = response.fine_tuned_model
                    job["trained_tokens"] = response.trained_tokens
                    job["error"] = response.error.to_dict() if response.error else None
                    job["updated_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    break

            save_finetune_jobs(jobs)
            print("ë¡œì»¬ ì‘ì—… ëª©ë¡ì´ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
        except Exception as local_update_error:
            print(f"âš ï¸ ë¡œì»¬ ëª©ë¡ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {local_update_error}")

        # ìƒíƒœ ì •ë³´ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜
        return {
            "id": response.id,
            "status": response.status,
            "model": response.model,
            "fine_tuned_model": response.fine_tuned_model,
            "created_at": response.created_at,
            "finished_at": response.finished_at,
            "trained_tokens": response.trained_tokens,
            "error": response.error.to_dict() if response.error else None,
        }

    except openai.APIError as e:
        if "not found" in str(e).lower():
            raise Exception(f"ì‘ì—… ID '{job_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        elif "insufficient_quota" in str(e):
            raise Exception("API ì‚¬ìš©ëŸ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ê²°ì œ ì •ë³´ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            raise Exception(f"OpenAI API ì˜¤ë¥˜: {str(e)}")
    except Exception as e:
        raise Exception(f"ìƒíƒœ í™•ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def upload_training_file(file_path):
    """OpenAIì— í›ˆë ¨ íŒŒì¼ ì—…ë¡œë“œ"""
    try:
        with open(file_path, "rb") as f:
            response = client.files.create(file=f, purpose="fine-tune")
        return response.id, None
    except Exception as e:
        return None, str(e)


def create_finetune_job(
    training_file_id, validation_file_id=None, model="gpt-3.5-turbo-1106"
):
    """íŒŒì¸íŠœë‹ ì‘ì—… ìƒì„±"""
    try:
        hyperparameters = {
            "n_epochs": st.session_state.get("finetune_epochs", 3),
            "batch_size": st.session_state.get("finetune_batch_size", 1),
            "learning_rate_multiplier": st.session_state.get(
                "finetune_learning_rate", 2.0
            ),
        }

        job_params = {
            "training_file": training_file_id,
            "model": model,
            "hyperparameters": hyperparameters,
        }

        if validation_file_id:
            job_params["validation_file"] = validation_file_id

        response = client.fine_tuning.jobs.create(**job_params)

        return response.id, None
    except Exception as e:
        return None, str(e)


def get_finetune_job_status(job_id):
    """íŒŒì¸íŠœë‹ ì‘ì—… ìƒíƒœ í™•ì¸"""
    try:
        response = client.fine_tuning.jobs.retrieve(job_id)
        return response, None
    except Exception as e:
        return None, str(e)


def list_finetune_jobs():
    try:
        response = client.fine_tuning.jobs.list()
        return response.data, None
    except Exception as e:
        return None, str(e)


def load_analysis_logs():
    """ë¶„ì„ ë¡œê·¸ CSVë¥¼ ì½ì–´ DataFrameìœ¼ë¡œ ë°˜í™˜"""

    columns = [
        "timestamp",  # ex: 2025-06-14T14:33:40.701977
        "date",  # ex: 2025-06-14
        "time",  # ex: 14:33:40
        "full_filename",  # ex: 20250614_143326_964332_camera_capture...
        "filename",  # ex: camera_capture_20250614_143326_382309.jpg
        "confidence_score",  # ex: 0.18
        "width",  # ex: 1280
        "height",  # ex: 720
        "model_used",  # ex: gpt-4o
        "description",  # ex: ì „ì²´ì ì¸ ë‚´ìš©...
        "judgment",  # ex: ë¶ˆëŸ‰
        "score",  # ex: 95.0
        "processing_time_seconds",  # ex: 3.98
        "defect_type",  # ex: ìƒ‰ìƒ
        "Unnamed: 14",  # ë¶ˆí•„ìš”, ì œê±° ê°€ëŠ¥
        "Unnamed: 15",
        "Unnamed: 16",
        "user_feedback",  # ex: No
    ]

    if not os.path.exists(LOG_CSV_PATH):
        return pd.DataFrame(columns=columns)

    try:
        # header=None, names=columns : CSVì— í—¤ë”ê°€ ì—†ê±°ë‚˜ ì»¬ëŸ¼ëª…ì„ ë®ì–´ì“¸ ë•Œ ì‚¬ìš©
        df = pd.read_csv(LOG_CSV_PATH, encoding="utf-8")
        return df
    except Exception as e:
        st.error(f"ë¡œê·¸ íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return pd.DataFrame(columns=columns)


def allowed_file(filename):
    """í—ˆìš©ëœ íŒŒì¼ í™•ì¥ì í™•ì¸"""
    return "." in filename and filename.rsplit(".", 1)[1].lower() in ALLOWED_EXTENSIONS


# Flask API ì—”ë“œí¬ì¸íŠ¸ë“¤
@flask_app.route("/upload", methods=["POST"])
def upload_image():
    """ì´ë¯¸ì§€ ì—…ë¡œë“œ API"""
    try:
        if "image" not in request.files:
            return jsonify({"error": "No image file provided"}), 400

        file = request.files["image"]
        if file.filename == "":
            return jsonify({"error": "No file selected"}), 400

        if file and allowed_file(file.filename):
            # ì•ˆì „í•œ íŒŒì¼ëª… ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
            filename = f"{timestamp}_{secure_filename(file.filename)}"
            filepath = os.path.join(flask_app.config["UPLOAD_FOLDER"], filename)

            # íŒŒì¼ ì €ì¥
            file.save(filepath)

            # íì— ìƒˆ ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
            image_queue.put(
                {
                    "filename": filename,
                    "filepath": filepath,
                    "timestamp": timestamp,
                    "original_name": file.filename,
                    "upload_time": datetime.now().isoformat(),
                }
            )

            return (
                jsonify(
                    {
                        "message": "Image uploaded successfully",
                        "filename": filename,
                        "timestamp": timestamp,
                    }
                ),
                200,
            )
        else:
            return jsonify({"error": "Invalid file type"}), 400

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@flask_app.route("/images/<path:filename>")
def serve_image(filename):
    try:
        response = make_response(
            send_from_directory(flask_app.config["UPLOAD_FOLDER"], filename)
        )
        response.headers["Access-Control-Allow-Origin"] = "*"
        return response
    except Exception as e:
        return jsonify({"error": f"Cannot serve image: {str(e)}"}), 404


@flask_app.route("/upload_base64", methods=["POST"])
def upload_base64_image():
    """Base64 ì¸ì½”ë”©ëœ ì´ë¯¸ì§€ ì—…ë¡œë“œ API"""
    try:
        data = request.get_json()
        if "image" not in data:
            return jsonify({"error": "No image data provided"}), 400

        # Base64 ë””ì½”ë”©
        image_data = data["image"]
        if "data:image" in image_data:
            # data URL í˜•ì‹ì¸ ê²½ìš° í—¤ë” ì œê±°
            image_data = image_data.split(",")[1]

        # ì´ë¯¸ì§€ ë””ì½”ë”© ë° ì €ì¥
        image_bytes = base64.b64decode(image_data)
        image = Image.open(io.BytesIO(image_bytes))

        # íŒŒì¼ëª… ìƒì„±
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S_%f")
        filename = f"{timestamp}_camera_capture.jpg"
        filepath = os.path.join(flask_app.config["UPLOAD_FOLDER"], filename)

        # ì´ë¯¸ì§€ ì €ì¥
        image.save(filepath, "JPEG")

        # íì— ìƒˆ ì´ë¯¸ì§€ ì •ë³´ ì¶”ê°€
        image_queue.put(
            {
                "filename": filename,
                "filepath": filepath,
                "timestamp": timestamp,
                "original_name": "camera_capture.jpg",
                "upload_time": datetime.now().isoformat(),
            }
        )

        return (
            jsonify(
                {
                    "message": "Image uploaded successfully",
                    "filename": filename,
                    "timestamp": timestamp,
                }
            ),
            200,
        )

    except Exception as e:
        return jsonify({"error": str(e)}), 500


@flask_app.route("/status", methods=["GET"])
def get_status():
    """ì„œë²„ ìƒíƒœ í™•ì¸ API"""
    return jsonify(
        {
            "status": "running",
            "timestamp": datetime.now().isoformat(),
            "queue_size": image_queue.qsize(),
        }
    )


@flask_app.route("/health", methods=["GET"])
def health_check():
    """í—¬ìŠ¤ì²´í¬ API"""
    return jsonify(
        {
            "status": "healthy",
            "server": "AI Image Analyzer",
            "version": "3.0",
            "timestamp": datetime.now().isoformat(),
        }
    )


def run_flask_server():
    """Flask ì„œë²„ ì‹¤í–‰"""
    flask_app.run(host="0.0.0.0", port=5000, debug=False, use_reloader=False)


# ì„œë²„ ì •ë³´ í‘œì‹œ í•¨ìˆ˜
def get_server_info():
    """í˜„ì¬ ì„œë²„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°"""
    hostname = socket.gethostname()
    try:
        # ë” ì •í™•í•œ ë¡œì»¬ IP ê°€ì ¸ì˜¤ê¸°
        s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        s.connect(("8.8.8.8", 80))
        local_ip = s.getsockname()[0]
        s.close()
    except:
        local_ip = socket.gethostbyname(hostname)
    return hostname, local_ip


def get_received_images():
    """ë°›ì€ ì´ë¯¸ì§€ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    image_files = []
    for ext in ["*.jpg", "*.jpeg", "*.png"]:
        image_files.extend(glob.glob(os.path.join(UPLOAD_FOLDER, ext)))

    # ìµœì‹  ìˆœìœ¼ë¡œ ì •ë ¬
    image_files.sort(key=os.path.getctime, reverse=True)
    return image_files


def generate_image_analysis(image_path, system_prompt, model_name="gpt-4o"):
    """ì´ë¯¸ì§€ ë¶„ì„ ìˆ˜í–‰"""
    start_time = time.time()

    try:
        llm = ChatOpenAI(
            temperature=0,
            model_name=model_name,
        )

        # ê¸°ë³¸ ë¶„ì„ í”„ë¡¬í”„íŠ¸
        user_prompt = """
        ì´ ì´ë¯¸ì§€ë¥¼ ìì„¸íˆ ë¶„ì„í•´ ì£¼ì„¸ìš”. ë‹¤ìŒ í•­ëª©ë“¤ì„ í¬í•¨í•´ì„œ ì„¤ëª…í•´ ì£¼ì„¸ìš”:
        
        1. **ì „ì²´ì ì¸ ë‚´ìš©**: ì´ë¯¸ì§€ì—ì„œ ë³´ì´ëŠ” ë¬¼ì²´
        2. **ê°ì²´ ë° ìš”ì†Œ**: ì´ë¯¸ì§€ì— ë¬¼ì²´ë¥¼ ê°ì‹¸ê³  ìˆëŠ” ì¢…ì´ì˜ ìƒ‰ê¹”
        3. **í’ˆì§ˆ íŒë‹¨**: ë¶ˆëŸ‰/ì •í’ˆ íŒì • ê²°ê³¼
        4. **ì‹ ë¢°ë„**: íŒì •ì— ëŒ€í•œ í™•ì‹  ì •ë„ (%)

        """

        multimodal_llm = MultiModal(
            llm, system_prompt=system_prompt, user_prompt=user_prompt
        )

        # ì‘ë‹µ ë°›ê¸° ë° ë‚´ìš© ì¶”ì¶œ
        response = multimodal_llm.invoke(image_path)

        processing_time = time.time() - start_time

        # ì‘ë‹µ íƒ€ì…ì— ë”°ë¥¸ ì²˜ë¦¬
        if hasattr(response, "content"):
            # AIMessage ê°ì²´ì¸ ê²½ìš°
            result = response.content
        elif isinstance(response, str):
            # ì´ë¯¸ ë¬¸ìì—´ì¸ ê²½ìš°
            result = response
        else:
            # ê¸°íƒ€ ê²½ìš° - ë¬¸ìì—´ë¡œ ë³€í™˜ ì‹œë„
            result = str(response)

        return result, processing_time

    except Exception as e:
        processing_time = time.time() - start_time

        # ë” ìì„¸í•œ ì—ëŸ¬ ì •ë³´ ì œê³µ
        error_msg = f"âŒ ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n"
        error_msg += f"ì˜¤ë¥˜ ìœ í˜•: {type(e).__name__}\n"
        error_msg += f"ì˜¤ë¥˜ ë‚´ìš©: {str(e)}\n"
        error_msg += f"ì´ë¯¸ì§€ ê²½ë¡œ: {image_path}\n"
        error_msg += f"ëª¨ë¸: {model_name}"

        # ë¡œê·¸ì—ë„ ê¸°ë¡
        st.error(f"Image analysis error: {str(e)}")

        return error_msg, processing_time


def auto_analyze_new_images():
    """ìƒˆë¡œìš´ ì´ë¯¸ì§€ ìë™ ë¶„ì„"""
    if "analyzed_images" not in st.session_state:
        st.session_state.analyzed_images = set()

    if "analysis_results" not in st.session_state:
        st.session_state.analysis_results = {}

    received_images = get_received_images()

    for image_path in received_images[:5]:  # ìµœê·¼ 5ê°œë§Œ ìë™ ë¶„ì„
        if image_path not in st.session_state.analyzed_images:
            # ìë™ ë¶„ì„ ìˆ˜í–‰
            system_prompt = st.session_state.get(
                "system_prompt",
                "ë‹¹ì‹ ì€ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ë„ì¥ ë¶ˆëŸ‰ë¥ ì„ íŒë‹¨í•˜ëŠ” ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. ì´ë¯¸ì§€ì˜ ë¬¼ì²´ê°€ ë¹¨ê°„ìƒ‰ ì¢…ì´ë¡œ ê°ì‹¸ì ¸ ìˆìœ¼ë©´ ë¶ˆëŸ‰ìœ¼ë¡œ, íŒŒë€ìƒ‰ìœ¼ë¡œ ê°ì‹¸ì ¸ ìˆë‹¤ë©´ ì •í’ˆìœ¼ë¡œ íŒë‹¨í•´ì£¼ì„¸ìš”.",
            )

            model_name = st.session_state.get("selected_model", "gpt-4o")

            result, processing_time = generate_image_analysis(
                image_path, system_prompt, model_name
            )

            # ê²°ê³¼ ì €ì¥
            analysis_info = {
                "result": result,
                "timestamp": datetime.now(),
                "model": model_name,
                "processing_time": processing_time,
            }

            st.session_state.analysis_results[image_path] = analysis_info
            st.session_state.analyzed_images.add(image_path)

            # CSV ë¡œê·¸ì— ê¸°ë¡
            original_name = (
                os.path.basename(image_path).split("_", 3)[-1]
                if "_" in os.path.basename(image_path)
                else os.path.basename(image_path)
            )
            log_analysis_result(
                image_path, original_name, result, model_name, processing_time
            )


# CSV ë¡œê·¸ íŒŒì¼ ì´ˆê¸°í™”
initialize_csv_log()
initialize_finetune_jobs()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="AI ì´ë¯¸ì§€ ë¶„ì„ê¸° with íŒŒì¸íŠœë‹ ",
    page_icon="ğŸ¯",
    layout="wide",
    initial_sidebar_state="expanded",
)

# íƒ­ êµ¬ì„±
tab1, tab2, tab3, tab4, tab5 = st.tabs(
    ["ğŸ“¡ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§", "ğŸ“Š ë°ì´í„° ë¶„ì„", "ğŸ”§ íŒŒì¸íŠœë‹", "ğŸ“ í”¼ë“œë°± ê´€ë¦¬", "âš™ï¸ ì„¤ì •"]
)


def delete_all_images():
    """ëª¨ë“  ì €ì¥ëœ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ"""
    try:
        deleted_count = 0

        # temp_captures í´ë”ì˜ ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
        if os.path.exists("temp_captures"):
            image_files = glob.glob("temp_captures/*")
            for file_path in image_files:
                if os.path.isfile(file_path) and file_path.lower().endswith(
                    (".jpg", ".jpeg", ".png", ".bmp", ".gif")
                ):
                    os.remove(file_path)
                    deleted_count += 1

        return deleted_count, None
    except Exception as e:
        return 0, str(e)


def refresh_received_images():
    """íŒŒì¼ ì‹œìŠ¤í…œê³¼ ì„¸ì…˜ ìƒíƒœ ë™ê¸°í™”"""
    try:
        folder = UPLOAD_FOLDER  # "received_images"ê°€ ë  ê²ƒ
        if os.path.exists(folder):
            actual_files = []
            for file_path in glob.glob(os.path.join(folder, "*")):
                if os.path.isfile(file_path) and file_path.lower().endswith(
                    (".jpg", ".jpeg", ".png", ".bmp", ".gif")
                ):
                    actual_files.append(file_path)

            # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
            if "received_images" in st.session_state:
                st.session_state.received_images = [
                    img
                    for img in st.session_state.received_images
                    if img in actual_files
                ]

            if "analysis_results" in st.session_state:
                existing_results = {
                    img_path: result
                    for img_path, result in st.session_state.analysis_results.items()
                    if img_path in actual_files
                }
                st.session_state.analysis_results = existing_results

            return actual_files
        else:
            return []
    except Exception as e:
        st.error(f"íŒŒì¼ ë™ê¸°í™” ì˜¤ë¥˜: {str(e)}")
        return []


def delete_selected_images(image_paths):
    """ì„ íƒëœ ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì‚­ì œ"""
    try:
        deleted_count = 0
        failed_files = []

        for img_path in image_paths:
            try:
                if os.path.exists(img_path):
                    os.remove(img_path)
                    deleted_count += 1
            except Exception as e:
                failed_files.append(f"{img_path}: {str(e)}")

        return deleted_count, failed_files
    except Exception as e:
        return 0, [str(e)]


def clear_all_analysis_logs():
    """ëª¨ë“  ë¶„ì„ ë¡œê·¸ë¥¼ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜ (ë¹ˆ CSVë¡œ ì´ˆê¸°í™”)"""
    try:
        empty_df = pd.DataFrame(
            columns=[
                "timestamp",
                "filename",
                "judgment",
                "confidence_score",
                "processing_time_seconds",
                "model_used",
                "user_feedback",
                "date",
                "defect_type",
            ]
        )
        empty_df.to_csv(LOG_CSV_PATH, index=False)
        return True, None
    except Exception as e:
        return False, str(e)


def clear_temp_folder():
    """temp_captures í´ë” ì „ì²´ ì‚­ì œ í›„ ì¬ìƒì„±"""
    try:
        if os.path.exists("temp_captures"):
            shutil.rmtree("temp_captures")

        os.makedirs("temp_captures", exist_ok=True)
        return True, None
    except Exception as e:
        return False, str(e)


# ê¸°ì¡´ ì½”ë“œì— ì¶”ê°€í•  ë¶€ë¶„
with tab1:
    # ë©”ì¸ ì œëª©
    st.title("AI ì´ë¯¸ì§€ ë¶„ì„")
    st.markdown("### ì´ë¯¸ì§€ ë°ì´í„° ê°€ì‹œí™” ë° íŒŒì¸íŠœë‹")

    # ì„œë²„ ì •ë³´ í‘œì‹œ
    hostname, local_ip = get_server_info()
    st.info(f"ğŸŒ ì„œë²„ ì •ë³´ - í˜¸ìŠ¤íŠ¸: {hostname} | ì ‘ì† IP: {local_ip}")

    # ì´ë¯¸ì§€ ê´€ë¦¬ ë²„íŠ¼ë“¤ (ìƒë‹¨ì— ë°°ì¹˜)
    col1, col2, col3 = st.columns([1, 1, 2])

    with col1:
        if st.button("ğŸ—‘ï¸ ëª¨ë“  ì´ë¯¸ì§€ ì‚­ì œ", type="secondary"):
            deleted_count, error = delete_all_images()
            if error:
                st.error(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {error}")
            else:
                # ì„¸ì…˜ ìƒíƒœ ì™„ì „ ì´ˆê¸°í™”
                st.session_state.received_images = []
                st.session_state.analysis_results = {}
                st.session_state.analyzed_images = set()

                # íë„ ë¹„ìš°ê¸°
                while not image_queue.empty():
                    try:
                        image_queue.get_nowait()
                    except queue.Empty:
                        break

                st.success(f"âœ… {deleted_count}ê°œ ì´ë¯¸ì§€ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.rerun()

    with col2:
        if st.button("ğŸ”„ í´ë” ì´ˆê¸°í™”", type="secondary"):
            success, error = clear_temp_folder()
            if error:
                st.error(f"âŒ ì´ˆê¸°í™” ì‹¤íŒ¨: {error}")
            else:
                # ì„¸ì…˜ ìƒíƒœ ì™„ì „ ì´ˆê¸°í™”
                st.session_state.received_images = []
                st.session_state.analysis_results = {}
                st.session_state.analyzed_images = set()

                # íë„ ë¹„ìš°ê¸°
                while not image_queue.empty():
                    try:
                        image_queue.get_nowait()
                    except queue.Empty:
                        break

                st.success("âœ… ì´ë¯¸ì§€ í´ë”ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.rerun()

    # íŒŒì¼ ì‹œìŠ¤í…œê³¼ ì„¸ì…˜ ìƒíƒœ ë™ê¸°í™”
    actual_files = refresh_received_images()

    # Flask ì„œë²„ ì‹œì‘ (ì„¸ì…˜ ìƒíƒœë¡œ ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€)
    if "flask_started" not in st.session_state:
        try:
            flask_thread = threading.Thread(target=run_flask_server, daemon=True)
            flask_thread.start()
            st.session_state.flask_started = True
            st.success("ğŸš€ ì´ë¯¸ì§€ ìˆ˜ì‹  ì„œë²„ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
        except Exception as e:
            st.error(f"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {str(e)}")

    # ìƒˆë¡œìš´ ì´ë¯¸ì§€ ì²´í¬ ë° ì²˜ë¦¬
    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "received_images" not in st.session_state:
        st.session_state.received_images = []

    # ì´ë¯¸ì§€ í ìˆ˜ì‹ 
    new_images = []
    while True:
        try:
            new_image = image_queue.get_nowait()
            new_images.append(new_image)
            st.session_state.received_images.insert(0, new_image)
        except queue.Empty:
            break

    # ìë™ ë¶„ì„ ì„¤ì • (ì‚¬ì´ë“œë°”ì—ì„œ ê°€ì ¸ì˜¤ê¸°)
    auto_analysis = st.session_state.get("auto_analysis", True)

    # ìë™ ë¶„ì„ ìˆ˜í–‰
    if auto_analysis:
        auto_analyze_new_images()

    # ìƒˆ ì´ë¯¸ì§€ê°€ ë„ì°©í–ˆì„ ë•Œ ì•Œë¦¼
    if new_images:
        st.success(f"ğŸ†• ìƒˆë¡œìš´ ì´ë¯¸ì§€ {len(new_images)}ê°œê°€ ë„ì°©í–ˆìŠµë‹ˆë‹¤!")
        for img_info in new_images:
            st.info(f"ğŸ“· {img_info['original_name']} ({img_info['timestamp']})")

    # ë°›ì€ ì´ë¯¸ì§€ ëª©ë¡ í‘œì‹œ (íŒŒì¼ ì‹œìŠ¤í…œê³¼ ë™ê¸°í™”ëœ ëª©ë¡ ì‚¬ìš©)
    received_images = actual_files  # get_received_images() ëŒ€ì‹  actual_files ì‚¬ìš©

    if received_images:
        st.write(f"**ì´ {len(received_images)}ê°œì˜ ì´ë¯¸ì§€ë¥¼ ë°›ì•˜ìŠµë‹ˆë‹¤**")

        # ê°œë³„ ì‚­ì œë¥¼ ìœ„í•œ ì„ íƒ ì˜µì…˜
        st.markdown("#### ê°œë³„ ì´ë¯¸ì§€ ì‚­ì œ")
        selected_for_deletion = []

        # ìµœê·¼ ì´ë¯¸ì§€ë“¤ì„ ê·¸ë¦¬ë“œë¡œ í‘œì‹œ
        cols = st.columns(3)

        for idx, img_path in enumerate(received_images[:9]):  # ìµœê·¼ 9ê°œë§Œ í‘œì‹œ
            with cols[idx % 3]:
                # ì‚­ì œ ì„ íƒ ì²´í¬ë°•ìŠ¤
                delete_selected = st.checkbox(
                    f"ì‚­ì œ ì„ íƒ",
                    key=f"delete_check_{idx}",
                    help="ì´ ì´ë¯¸ì§€ë¥¼ ì‚­ì œí•˜ë ¤ë©´ ì²´í¬í•˜ì„¸ìš”",
                )

                if delete_selected:
                    selected_for_deletion.append(img_path)

                # ì´ë¯¸ì§€ í‘œì‹œ
                st.image(
                    img_path,
                    caption=f"{os.path.basename(img_path)}",
                    use_column_width=True,
                )

                # ë¶„ì„ ê²°ê³¼ê°€ ìˆìœ¼ë©´ í‘œì‹œ
                if img_path in st.session_state.get("analysis_results", {}):
                    analysis_info = st.session_state.analysis_results[img_path]

                    # ë¶ˆëŸ‰/ì •í’ˆ íŒì • ê²°ê³¼ í•˜ì´ë¼ì´íŠ¸
                    result_text = analysis_info["result"]
                    if "ë¶ˆëŸ‰" in result_text:
                        st.error("ğŸš¨ ë¶ˆëŸ‰ íŒì •")
                    elif "ì •í’ˆ" in result_text or "ì •ìƒ" in result_text:
                        st.success("âœ… ì •í’ˆ íŒì •")
                    else:
                        st.info("ğŸ” ë¶„ì„ ì™„ë£Œ")

                    # ìƒì„¸ ê²°ê³¼ ë³´ê¸°
                    with st.expander(f"ìƒì„¸ ê²°ê³¼ ë³´ê¸°", expanded=False):
                        st.write(
                            f"**ë¶„ì„ ì‹œê°„:** {analysis_info['timestamp'].strftime('%Y-%m-%d %H:%M:%S')}"
                        )
                        st.write(
                            f"**ì²˜ë¦¬ ì‹œê°„:** {analysis_info['processing_time']:.2f}ì´ˆ"
                        )
                        st.write("**ë¶„ì„ ê²°ê³¼:**")
                        st.write(result_text)

                # ìˆ˜ë™ ë¶„ì„ ë²„íŠ¼
                if st.button(f"ğŸ” ìˆ˜ë™ ë¶„ì„", key=f"analyze_{idx}"):
                    system_prompt = st.session_state.get(
                        "system_prompt",
                        "ë‹¹ì‹ ì€ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ë„ì¥ ë¶ˆëŸ‰ë¥ ì„ íŒë‹¨í•˜ëŠ” ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.",
                    )
                    model_name = st.session_state.get("selected_model", "gpt-4o")

                    with st.spinner("ë¶„ì„ ì¤‘..."):
                        result, processing_time = generate_image_analysis(
                            img_path, system_prompt, model_name
                        )

                        # ê²°ê³¼ ì €ì¥
                        analysis_info = {
                            "result": result,
                            "timestamp": datetime.now(),
                            "model": model_name,
                            "processing_time": processing_time,
                        }

                        if "analysis_results" not in st.session_state:
                            st.session_state.analysis_results = {}

                        st.session_state.analysis_results[img_path] = analysis_info
                        st.session_state.analyzed_images.add(img_path)

                        # CSV ë¡œê·¸ì— ê¸°ë¡
                        original_name = (
                            os.path.basename(img_path).split("_", 3)[-1]
                            if "_" in os.path.basename(img_path)
                            else os.path.basename(img_path)
                        )
                        log_analysis_result(
                            img_path, original_name, result, model_name, processing_time
                        )

                        st.rerun()

        # ì„ íƒëœ ì´ë¯¸ì§€ë“¤ ì‚­ì œ ë²„íŠ¼
        if selected_for_deletion:
            st.markdown("---")
            col_del1, col_del2 = st.columns([1, 3])

            with col_del1:
                if st.button(
                    f"ğŸ—‘ï¸ ì„ íƒëœ {len(selected_for_deletion)}ê°œ ì‚­ì œ", type="primary"
                ):
                    deleted_count, failed_files = delete_selected_images(
                        selected_for_deletion
                    )

                    if failed_files:
                        st.error(f"âŒ ì¼ë¶€ íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {failed_files}")

                    if deleted_count > 0:
                        # íŒŒì¼ ì‹œìŠ¤í…œê³¼ ì„¸ì…˜ ìƒíƒœ ë‹¤ì‹œ ë™ê¸°í™”
                        refresh_received_images()

                        st.success(f"âœ… {deleted_count}ê°œ ì´ë¯¸ì§€ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.rerun()

    else:
        st.info(
            "ğŸ“· ì•„ì§ ë°›ì€ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤. ì¹´ë©”ë¼ë‚˜ APIë¥¼ í†µí•´ ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•´ ì£¼ì„¸ìš”."
        )

        # API ì‚¬ìš© ì˜ˆì‹œ
        st.markdown("### ğŸ“¡ API ì‚¬ìš© ë°©ë²•")
        st.code(
            f"""
http:// ì›¹ ì‚¬ì´íŠ¸ì— ì…ë ¥
        """,
            language="bash",
        )


def delete_all_analysis_data():
    """ì „ì²´ ì‚­ì œë¥¼ ìœ„í•œ ë˜í¼ í•¨ìˆ˜"""
    success, error = clear_all_analysis_logs()
    return success, error


def delete_selected_analysis_data(timestamps):
    """ì„ íƒëœ timestamp ë¦¬ìŠ¤íŠ¸ì— í•´ë‹¹í•˜ëŠ” ë¡œê·¸ ì‚­ì œ í•¨ìˆ˜"""
    try:
        df = load_analysis_logs()
        if df.empty:
            return 0, None
        initial_count = len(df)
        df_filtered = df[~df["timestamp"].isin(timestamps)]
        df_filtered.to_csv(LOG_CSV_PATH, index=False)
        deleted_count = initial_count - len(df_filtered)
        return deleted_count, None
    except Exception as e:
        return 0, str(e)


def refresh_analysis_data():
    """ë¶„ì„ ë¡œê·¸ë¥¼ ìµœì‹ ìœ¼ë¡œ ë‹¤ì‹œ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ (íƒ­2ì—ì„œ í˜¸ì¶œìš©)"""
    return load_analysis_logs()


with tab2:
    st.header("ë°ì´í„° ë¶„ì„ ëŒ€ì‹œë³´ë“œ")

    # ë¶„ì„ ë¡œê·¸ ë¡œë“œ (ìƒˆë¡œê³ ì¹¨ í•¨ìˆ˜ ì‚¬ìš©)
    df = refresh_analysis_data()

    if not df.empty:
        # ìƒë‹¨ì— ì „ì²´ ì‚­ì œ ë²„íŠ¼ ì¶”ê°€
        col_delete, col_refresh, col_info = st.columns([1, 1, 2])

        with col_delete:
            if st.button(
                "ğŸ—‘ï¸ ì „ì²´ ë°ì´í„° ì‚­ì œ",
                type="secondary",
                help="ëª¨ë“  ë¶„ì„ ë°ì´í„°ë¥¼ ì‚­ì œí•©ë‹ˆë‹¤",
            ):
                if st.session_state.get("confirm_delete_all_data", False):
                    success, error = delete_all_analysis_data()
                    if success:
                        if "items_to_delete" in st.session_state:
                            st.session_state.items_to_delete = []
                        if "confirm_delete_all_data" in st.session_state:
                            del st.session_state["confirm_delete_all_data"]
                        if "confirm_delete_selected" in st.session_state:
                            del st.session_state["confirm_delete_selected"]
                        st.success("âœ… ëª¨ë“  ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤!")
                        st.rerun()
                    else:
                        st.error(f"âŒ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error}")
                else:
                    st.session_state["confirm_delete_all_data"] = True
                    st.warning(
                        "âš ï¸ ì •ë§ë¡œ ëª¨ë“  ë°ì´í„°ë¥¼ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? ë‹¤ì‹œ í•œ ë²ˆ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”."
                    )

        with col_refresh:
            if st.button("ğŸ”„ ë°ì´í„° ìƒˆë¡œê³ ì¹¨", type="secondary"):
                st.rerun()

        with col_info:
            st.write(f"**ì´ {len(df)}ê°œì˜ ë¶„ì„ ê¸°ë¡**")

        # ê¸°ë³¸ í†µê³„
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("ì´ ë¶„ì„ ê±´ìˆ˜", len(df))

        with col2:
            defect_count = len(df[df["judgment"] == "ë¶ˆëŸ‰"])
            st.metric("ë¶ˆëŸ‰ ê±´ìˆ˜", defect_count)

        with col3:
            normal_count = len(df[df["judgment"] == "ì •í’ˆ"])
            st.metric("ì •í’ˆ ê±´ìˆ˜", normal_count)

        with col4:
            if len(df) > 0:
                defect_rate = (defect_count / len(df)) * 100
                st.metric("ë¶ˆëŸ‰ë¥ ", f"{defect_rate:.1f}%")

        # ì‹œê³„ì—´ ì°¨íŠ¸
        st.subheader("ğŸ“ˆ ì‹œê°„ë³„ ë¶„ì„ í˜„í™©")
        df["date"] = pd.to_datetime(df["timestamp"], errors="coerce")
        df = df.dropna(subset=["date"])
        daily_stats = (
            df.groupby(df["date"].dt.date)
            .agg({"judgment": "count", "processing_time_seconds": "mean"})
            .rename(columns={"judgment": "count"})
        )

        if not daily_stats.empty:
            fig_timeline = px.line(
                daily_stats.reset_index(),
                x="date",
                y="count",
                title="ì¼ë³„ ë¶„ì„ ê±´ìˆ˜",
                markers=True,
            )
            st.plotly_chart(fig_timeline, use_container_width=True)

        # íŒì • ê²°ê³¼ ë¶„í¬
        st.subheader("íŒì • ê²°ê³¼ ë¶„í¬")
        col1, col2 = st.columns(2)

        with col1:
            judgment_counts = df["judgment"].value_counts()
            if not judgment_counts.empty:
                fig_pie = px.pie(
                    values=judgment_counts.values,
                    names=judgment_counts.index,
                    title="íŒì • ê²°ê³¼ ë¶„í¬",
                )
                st.plotly_chart(fig_pie, use_container_width=True)

        with col2:
            if "defect_type" in df.columns:
                defect_df = df[df["judgment"] == "ë¶ˆëŸ‰"]
                if not defect_df.empty:
                    defect_counts = defect_df["defect_type"].value_counts()
                    fig_defect = px.bar(
                        x=defect_counts.index,
                        y=defect_counts.values,
                        title="ë¶ˆëŸ‰ ìœ í˜• ë¶„í¬",
                    )
                    st.plotly_chart(fig_defect, use_container_width=True)

        # ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ
        if "model_used" in df.columns:
            st.subheader("ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµ")
            model_stats = (
                df.groupby("model_used")
                .agg(
                    {
                        "processing_time_seconds": "mean",
                        "judgment": "count",
                        "confidence_score": "mean",
                    }
                )
                .round(2)
            )

            st.dataframe(model_stats)

        # ìƒì„¸ ë¡œê·¸ í…Œì´ë¸”
        st.subheader("ğŸ“‹ ìƒì„¸ ë¶„ì„ ë¡œê·¸")

        # í•„í„°ë§ ì˜µì…˜
        col1, col2, col3 = st.columns(3)

        with col1:
            judgment_filter = st.selectbox(
                "íŒì • ê²°ê³¼ í•„í„°", options=["ì „ì²´"] + list(df["judgment"].unique())
            )

        with col2:
            date_filter = st.date_input(
                "ë‚ ì§œ í•„í„° (ì´í›„)",
                value=(
                    pd.to_datetime(df["timestamp"]).min().date()
                    if len(df) > 0
                    else datetime.now().date()
                ),
            )

        with col3:
            show_feedback_only = st.checkbox("í”¼ë“œë°± ìˆëŠ” í•­ëª©ë§Œ")

        # í•„í„° ì ìš©
        filtered_df = df.copy()

        if judgment_filter != "ì „ì²´":
            filtered_df = filtered_df[filtered_df["judgment"] == judgment_filter]

        if len(filtered_df) > 0:
            filtered_df["timestamp_date"] = pd.to_datetime(
                filtered_df["timestamp"]
            ).dt.date
            filtered_df = filtered_df[filtered_df["timestamp_date"] >= date_filter]

        if show_feedback_only and "user_feedback" in filtered_df.columns:
            filtered_df = filtered_df[
                filtered_df["user_feedback"].notna()
                & (filtered_df["user_feedback"] != "")
            ]

        # í…Œì´ë¸” í‘œì‹œ (ê°œë³„ ì‚­ì œ ê¸°ëŠ¥ í¬í•¨)
        if not filtered_df.empty:
            display_columns = [
                "timestamp",
                "filename",
                "judgment",
                "confidence_score",
                "processing_time_seconds",
                "model_used",
                "user_feedback",
            ]
            available_columns = [
                col for col in display_columns if col in filtered_df.columns
            ]

            # ê°œë³„ ì‚­ì œë¥¼ ìœ„í•œ ì²´í¬ë°•ìŠ¤ ì¶”ê°€
            st.write("**ê°œë³„ í•­ëª© ì‚­ì œ:**")

            # ì‚­ì œí•  í•­ëª©ë“¤ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
            if "items_to_delete" not in st.session_state:
                st.session_state.items_to_delete = []

            # ì „ì²´ ì„ íƒ/í•´ì œ ë° ì‚­ì œ ë²„íŠ¼
            col_select_all, col_delete_selected, col_clear_selection = st.columns(
                [1, 1, 1]
            )

            with col_select_all:
                select_all = st.checkbox("ì „ì²´ ì„ íƒ")
                if select_all:
                    # í˜„ì¬ í•„í„°ëœ ë°ì´í„°ì˜ ëª¨ë“  timestamp ì„ íƒ
                    st.session_state.items_to_delete = filtered_df["timestamp"].tolist()

            with col_clear_selection:
                if st.button("ğŸ”„ ì„ íƒ í•´ì œ", type="secondary"):
                    st.session_state.items_to_delete = []
                    if "confirm_delete_selected" in st.session_state:
                        del st.session_state["confirm_delete_selected"]
                    st.rerun()

            with col_delete_selected:
                if st.button("ğŸ—‘ï¸ ì„ íƒëœ í•­ëª© ì‚­ì œ", type="secondary"):
                    if st.session_state.items_to_delete:
                        if st.session_state.get("confirm_delete_selected", False):
                            # ì‹¤ì œ ì‚­ì œ ì‹¤í–‰
                            deleted_count, error = delete_selected_analysis_data(
                                st.session_state.items_to_delete
                            )

                            if error:
                                st.error(f"âŒ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {error}")
                            else:
                                st.success(
                                    f"âœ… {deleted_count}ê°œ í•­ëª©ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤!"
                                )

                                # ìƒíƒœ ì´ˆê¸°í™”
                                st.session_state.items_to_delete = []
                                st.session_state["confirm_delete_selected"] = False
                                st.rerun()
                        else:
                            st.session_state["confirm_delete_selected"] = True
                            st.warning(
                                f"âš ï¸ ì •ë§ë¡œ ì„ íƒëœ {len(st.session_state.items_to_delete)}ê°œ í•­ëª©ì„ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? ë‹¤ì‹œ í•œ ë²ˆ ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”."
                            )
                    else:
                        st.warning("ì‚­ì œí•  í•­ëª©ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")

            # ì„ íƒëœ í•­ëª© ìˆ˜ í‘œì‹œ
            if st.session_state.items_to_delete:
                st.info(f"ğŸ“‹ ì„ íƒëœ í•­ëª©: {len(st.session_state.items_to_delete)}ê°œ")

            # ê° í–‰ì— ëŒ€í•œ ì²´í¬ë°•ìŠ¤ì™€ ë°ì´í„° í‘œì‹œ
            sorted_df = filtered_df[available_columns].sort_values(
                "timestamp", ascending=False
            )

            st.markdown("---")

            for idx, row in sorted_df.iterrows():
                col_check, col_data = st.columns([0.05, 0.95])

                with col_check:
                    # timestampë¥¼ ê³ ìœ  í‚¤ë¡œ ì‚¬ìš©
                    item_timestamp = row["timestamp"]

                    is_checked = item_timestamp in st.session_state.items_to_delete

                    if st.checkbox(
                        "",
                        key=f"delete_item_{item_timestamp}",
                        value=is_checked,
                        label_visibility="hidden",
                    ):
                        if item_timestamp not in st.session_state.items_to_delete:
                            st.session_state.items_to_delete.append(item_timestamp)
                    else:
                        if item_timestamp in st.session_state.items_to_delete:
                            st.session_state.items_to_delete.remove(item_timestamp)

                with col_data:
                    # í–‰ ë°ì´í„°ë¥¼ ì‚¬ìš©ì ì¹œí™”ì ìœ¼ë¡œ í‘œì‹œ
                    col_data1, col_data2 = st.columns([1, 2])

                    with col_data1:
                        st.write(f"**íŒŒì¼ëª…:** {row.get('filename', 'N/A')}")
                        st.write(f"**íŒì •:** {row.get('judgment', 'N/A')}")
                        st.write(f"**ëª¨ë¸:** {row.get('model_used', 'N/A')}")

                    with col_data2:
                        st.write(f"**ì‹œê°„:** {row.get('timestamp', 'N/A')}")
                        st.write(
                            f"**ì²˜ë¦¬ì‹œê°„:** {row.get('processing_time_seconds', 'N/A')}ì´ˆ"
                        )
                        if "confidence_score" in row and pd.notna(
                            row["confidence_score"]
                        ):
                            st.write(f"**ì‹ ë¢°ë„:** {row['confidence_score']}")
                        if (
                            "user_feedback" in row
                            and pd.notna(row["user_feedback"])
                            and row["user_feedback"]
                        ):
                            st.write(f"**í”¼ë“œë°±:** {row['user_feedback']}")

                st.divider()

            # CSV ë‹¤ìš´ë¡œë“œ
            st.markdown("---")
            csv = filtered_df.to_csv(index=False)
            st.download_button(
                label="ğŸ“¥ CSV ë‹¤ìš´ë¡œë“œ",
                data=csv,
                file_name=f"analysis_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv",
            )

        else:
            st.info("í•„í„° ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    else:
        st.info("ì•„ì§ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ë¥¼ ì—…ë¡œë“œí•˜ê³  ë¶„ì„ì„ ì‹œì‘í•´ì£¼ì„¸ìš”.")

        # ë¹ˆ CSV íŒŒì¼ì´ ìˆë‹¤ë©´ ì •ë¦¬
        csv_files = glob.glob(os.path.join(DATA_LOG_FOLDER, "*.csv"))
        for csv_file in csv_files:
            try:
                test_df = pd.read_csv(csv_file, encoding="utf-8")
                if len(test_df) == 0:
                    os.remove(csv_file)
                    st.info(f"ë¹ˆ ë°ì´í„° íŒŒì¼ '{csv_file}'ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")
            except Exception:
                pass


# ì¶”ê°€ë¡œ í•„ìš”í•œ í•¨ìˆ˜ (ê¸°ì¡´ ì½”ë“œì— ì—†ë‹¤ë©´ ì¶”ê°€)
def delete_analysis_record(timestamp_to_delete):
    """íŠ¹ì • timestampì˜ ë¶„ì„ ê¸°ë¡ì„ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜"""
    try:
        df = load_analysis_logs()
        if df.empty:
            return False, "ì‚­ì œí•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        df_filtered = df[df["timestamp"] != timestamp_to_delete]
        df_filtered.to_csv(LOG_CSV_PATH, index=False)
        return True, None
    except Exception as e:
        return False, str(e)


def import_job_to_local(job):
    try:
        jobs = load_finetune_jobs()

        # ì´ë¯¸ ë¡œì»¬ì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        if any(local_job["job_id"] == job.id for local_job in jobs):
            st.warning("ì´ë¯¸ ë¡œì»¬ì— ì €ì¥ëœ ì‘ì—…ì…ë‹ˆë‹¤.")
            return

        new_job = {
            "job_id": job.id,
            "model": job.model if hasattr(job, "model") else "",
            "status": job.status if hasattr(job, "status") else "unknown",
            "created_at": job.created_at if hasattr(job, "created_at") else "",
            "fine_tuned_model": getattr(job, "fine_tuned_model", None),
        }

        jobs.append(new_job)
        save_finetune_jobs(jobs)
        st.success(f"âœ… ì‘ì—… {job.id[:12]}... ê°€ ë¡œì»¬ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.error(f"ì‘ì—… ë¡œì»¬ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


def fetch_openai_jobs(limit: int = 20):
    """OpenAIì—ì„œ íŒŒì¸íŠœë‹ ì‘ì—… ëª©ë¡ì„ ê°€ì ¸ì™€ ì„¸ì…˜ ìƒíƒœì™€ ë¡œì»¬ì— ì €ì¥"""
    try:
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise Exception(
                "âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            )

        client = openai.OpenAI(api_key=api_key)
        print("OpenAIì—ì„œ íŒŒì¸íŠœë‹ ì‘ì—… ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
        response = client.fine_tuning.jobs.list(limit=limit)

        if response.data:
            updated_count = 0
            fetched_jobs = []
            for job in response.data:
                job_data = {
                    "job_id": job.id,
                    "model": job.model,
                    "status": job.status,
                    "created_at": datetime.fromtimestamp(job.created_at).strftime(
                        "%Y-%m-%d %H:%M:%S"
                    ),
                    "finished_at": (
                        datetime.fromtimestamp(job.finished_at).strftime(
                            "%Y-%m-%d %H:%M:%S"
                        )
                        if job.finished_at
                        else None
                    ),
                    "fine_tuned_model": job.fine_tuned_model,
                    "training_file": job.training_file,
                    "validation_file": job.validation_file,
                    "hyperparameters": (
                        job.hyperparameters.to_dict() if job.hyperparameters else {}
                    ),
                    "result_files": job.result_files,
                    "trained_tokens": job.trained_tokens,
                    "error": job.error.to_dict() if job.error else None,
                    "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                }
                fetched_jobs.append(job_data)
                if add_finetune_job(job_data):
                    updated_count += 1

            # âœ… ì„¸ì…˜ ìƒíƒœì— ì €ì¥ (ì „ì—­ì²˜ëŸ¼ ì‚¬ìš© ê°€ëŠ¥)
            st.session_state.finetune_jobs_json = fetched_jobs

            # âœ… ë¡œì»¬ JSON íŒŒì¼ë¡œë„ ì €ì¥
            save_finetune_jobs(fetched_jobs)

            print(
                f"âœ… {len(response.data)}ê°œì˜ ì‘ì—…ì„ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤. ({updated_count}ê°œ ì—…ë°ì´íŠ¸)"
            )

            # ìƒíƒœ í†µê³„ ì¶œë ¥
            status_counts = {}
            for job in fetched_jobs:
                status = job["status"]
                status_counts[status] = status_counts.get(status, 0) + 1

            if status_counts:
                print("\nğŸ“Š ì‘ì—… ìƒíƒœë³„ í†µê³„:")
                for status, count in status_counts.items():
                    emoji = {
                        "running": "ğŸŸ¡",
                        "succeeded": "ğŸŸ¢",
                        "failed": "ğŸ”´",
                        "cancelled": "âš«",
                    }.get(status, "âšª")
                    print(f"  {emoji} {status}: {count}ê°œ")

            print(f"\nğŸ“‹ ìµœê·¼ ì‘ì—… {min(5, len(fetched_jobs))}ê°œ:")
            for i, job in enumerate(fetched_jobs[:5]):
                emoji = {
                    "running": "ğŸŸ¡",
                    "succeeded": "ğŸŸ¢",
                    "failed": "ğŸ”´",
                    "cancelled": "âš«",
                }.get(job["status"], "âšª")
                print(
                    f"  {i+1}. {emoji} {job['job_id'][:20]}... | {job['status']} | {job['fine_tuned_model'] or job['model']}"
                )

            return {
                "total_jobs": len(fetched_jobs),
                "updated_jobs": updated_count,
                "status_counts": status_counts,
                "jobs": fetched_jobs,
            }

        else:
            st.session_state.finetune_jobs_json = []
            save_finetune_jobs([])  # ë¹ˆ ë¦¬ìŠ¤íŠ¸ë„ ì €ì¥
            return {"total_jobs": 0, "updated_jobs": 0, "status_counts": {}, "jobs": []}

    except openai.AuthenticationError:
        raise Exception("âŒ OpenAI API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    except openai.RateLimitError:
        raise Exception("âŒ API ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    except openai.APIError as e:
        raise Exception(f"âŒ OpenAI API ì˜¤ë¥˜: {str(e)}")
    except Exception as e:
        raise Exception(f"âŒ ì‘ì—… ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def clear_finetune_jobs_cache():
    """
    íŒŒì¸íŠœë‹ ì‘ì—… ëª©ë¡ ìºì‹œë¥¼ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜
    """
    try:
        # ìºì‹œ íŒŒì¼ ê²½ë¡œë“¤ ì •ì˜
        cache_files = [
            "finetune_jobs.json",
            "data/finetune_jobs.json",
            "cache/finetune_jobs.json",
            "logs/finetune_jobs.json",
        ]

        deleted_files = []

        # ê° ê²½ë¡œì—ì„œ ìºì‹œ íŒŒì¼ ì‚­ì œ ì‹œë„
        for file_path in cache_files:
            try:
                if os.path.exists(file_path):
                    os.remove(file_path)
                    deleted_files.append(file_path)
            except Exception as e:
                st.warning(f"âš ï¸ {file_path} ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")

        # ì„¸ì…˜ ìƒíƒœì—ì„œë„ ê´€ë ¨ ë°ì´í„° ì œê±°
        session_keys_to_clear = [
            "finetune_jobs",
            "openai_jobs",
            "cached_jobs",
            "jobs_last_updated",
        ]

        cleared_session_keys = []
        for key in session_keys_to_clear:
            if key in st.session_state:
                del st.session_state[key]
                cleared_session_keys.append(key)

        # ê²°ê³¼ ë©”ì‹œì§€
        if deleted_files or cleared_session_keys:
            message_parts = []
            if deleted_files:
                message_parts.append(f"íŒŒì¼ {len(deleted_files)}ê°œ ì‚­ì œ")
            if cleared_session_keys:
                message_parts.append(f"ì„¸ì…˜ ë°ì´í„° {len(cleared_session_keys)}ê°œ ì‚­ì œ")

            return True, f"ìºì‹œ ì‚­ì œ ì™„ë£Œ: {', '.join(message_parts)}"
        else:
            return True, "ì‚­ì œí•  ìºì‹œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤."

    except Exception as e:
        return False, f"ìºì‹œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def estimate_finetuning_cost(sample_count, model):
    """íŒŒì¸íŠœë‹ ë¹„ìš© ì¶”ì •"""
    # OpenAI íŒŒì¸íŠœë‹ ë¹„ìš© (2024ë…„ ê¸°ì¤€, ì‹¤ì œ ë¹„ìš©ì€ í™•ì¸ í•„ìš”)
    cost_per_1k = {
        "gpt-3.5-turbo-1106": 0.008,
        "gpt-4-turbo-preview": 0.03,
        "gpt-4o-mini": 0.012,
    }

    base_cost = cost_per_1k.get(model, 0.01)
    estimated_tokens = sample_count * 1000  # ìƒ˜í”Œë‹¹ í‰ê·  í† í° ìˆ˜ ì¶”ì •

    return (estimated_tokens / 1000) * base_cost


def update_all_job_status():
    """ëª¨ë“  ì‘ì—…ì˜ ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤."""
    try:
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
        api_key = os.getenv("OPENAI_API_KEY")

        if not api_key:
            raise Exception(
                "âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            )

        # ë¡œì»¬ ì‘ì—… ëª©ë¡ ë¡œë“œ
        jobs = load_finetune_jobs()
        if not jobs:
            print("ì—…ë°ì´íŠ¸í•  ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            return {"total_jobs": 0, "updated_jobs": 0, "failed_jobs": 0, "errors": []}

        # OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = openai.OpenAI(api_key=api_key)

        print(f"ëª¨ë“  ì‘ì—… ìƒíƒœë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ì¤‘... (ì´ {len(jobs)}ê°œ ì‘ì—…)")

        updated_count = 0
        failed_count = 0
        errors = []

        for i, job in enumerate(jobs, 1):
            try:
                job_id = job.get("job_id")
                if not job_id:
                    print(f"  {i}. ì‘ì—… IDê°€ ì—†ëŠ” í•­ëª© ê±´ë„ˆëœ€")
                    continue

                print(f"  {i}/{len(jobs)}. {job_id[:20]}... ìƒíƒœ í™•ì¸ ì¤‘...")

                # OpenAIì—ì„œ ìµœì‹  ìƒíƒœ ì¡°íšŒ
                updated_job = client.fine_tuning.jobs.retrieve(job_id)

                # ì´ì „ ìƒíƒœì™€ ë¹„êµ
                old_status = job.get("status")
                new_status = updated_job.status

                # ì‘ì—… ë°ì´í„° ì—…ë°ì´íŠ¸
                job["status"] = new_status
                job["finished_at"] = (
                    datetime.fromtimestamp(updated_job.finished_at).strftime(
                        "%Y-%m-%d %H:%M:%S"
                    )
                    if updated_job.finished_at
                    else None
                )
                job["fine_tuned_model"] = updated_job.fine_tuned_model
                job["trained_tokens"] = updated_job.trained_tokens
                job["error"] = (
                    updated_job.error.to_dict() if updated_job.error else None
                )
                job["updated_at"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

                # ìƒíƒœ ë³€ê²½ í™•ì¸
                if old_status != new_status:
                    status_emoji = {
                        "running": "ğŸŸ¡",
                        "succeeded": "ğŸŸ¢",
                        "failed": "ğŸ”´",
                        "cancelled": "âš«",
                    }.get(new_status, "âšª")
                    print(
                        f"    âœ… ìƒíƒœ ë³€ê²½: {old_status} â†’ {status_emoji} {new_status}"
                    )
                else:
                    print(f"    â– ìƒíƒœ ìœ ì§€: {new_status}")

                updated_count += 1

            except openai.APIError as api_error:
                error_msg = f"ì‘ì—… {job_id[:12]}... API ì˜¤ë¥˜: {str(api_error)}"
                print(f"    âš ï¸ {error_msg}")
                errors.append(error_msg)
                failed_count += 1
                continue
            except Exception as e:
                error_msg = f"ì‘ì—… {job_id[:12]}... ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {str(e)}"
                print(f"    âš ï¸ {error_msg}")
                errors.append(error_msg)
                failed_count += 1
                continue

        # ì—…ë°ì´íŠ¸ëœ ëª©ë¡ ì €ì¥
        try:
            save_finetune_jobs(jobs)
            print(f"âœ… {updated_count}ê°œ ì‘ì—…ì˜ ìƒíƒœê°€ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤.")

            if failed_count > 0:
                print(f"âš ï¸ {failed_count}ê°œ ì‘ì—… ì—…ë°ì´íŠ¸ ì‹¤íŒ¨")

            # ìƒíƒœë³„ í†µê³„ í‘œì‹œ
            status_counts = {}
            for job in jobs:
                status = job.get("status", "unknown")
                status_counts[status] = status_counts.get(status, 0) + 1

            if status_counts:
                print(f"\nğŸ“Š í˜„ì¬ ì‘ì—… ìƒíƒœë³„ í†µê³„:")
                for status, count in status_counts.items():
                    status_emoji = {
                        "running": "ğŸŸ¡",
                        "succeeded": "ğŸŸ¢",
                        "failed": "ğŸ”´",
                        "cancelled": "âš«",
                    }.get(status, "âšª")
                    print(f"  {status_emoji} {status}: {count}ê°œ")

            return {
                "total_jobs": len(jobs),
                "updated_jobs": updated_count,
                "failed_jobs": failed_count,
                "errors": errors,
                "status_counts": status_counts,
            }

        except Exception as save_error:
            raise Exception(f"ì‘ì—… ëª©ë¡ ì €ì¥ ì‹¤íŒ¨: {str(save_error)}")

    except openai.AuthenticationError:
        raise Exception("âŒ OpenAI API í‚¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    except openai.RateLimitError:
        raise Exception("âŒ API ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        raise Exception(f"âŒ ìƒíƒœ ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def save_job_info(
    job_id,
    base_model,
    sample_count,
    hyperparameters,
    training_file_id,
    validation_file_id=None,
):
    try:
        jobs = load_finetune_jobs()
        new_job = {
            "job_id": job_id,
            "base_model": base_model,
            "sample_count": sample_count,
            "hyperparameters": hyperparameters,
            "training_file_id": training_file_id,
            "validation_file_id": validation_file_id,
            "status": "pending",
            "created_at": time.strftime("%Y-%m-%d %H:%M:%S"),
        }
        jobs.append(new_job)
        save_finetune_jobs(jobs)
    except Exception as e:
        st.error(f"íŒŒì¸íŠœë‹ ì‘ì—… ì •ë³´ ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")


def start_finetuning_process(
    sample_count, base_model, epochs, batch_size, learning_rate
):
    """íŒŒì¸íŠœë‹ í”„ë¡œì„¸ìŠ¤ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."""
    try:
        if not st.session_state.get("openai_api_key"):
            st.error("âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return

        client = openai.OpenAI(api_key=st.session_state.openai_api_key)

        with st.spinner("íŒŒì¸íŠœë‹ì„ ì‹œì‘í•˜ëŠ” ì¤‘..."):
            # ì—¬ê¸°ì„œ ì‹¤ì œ íŒŒì¸íŠœë‹ ì‘ì—…ì„ ì‹œì‘í•˜ëŠ” ì½”ë“œë¥¼ êµ¬í˜„
            # (í›ˆë ¨ íŒŒì¼ ì—…ë¡œë“œ, íŒŒì¸íŠœë‹ ì‘ì—… ìƒì„± ë“±)

            # ì˜ˆì‹œ ì‘ì—… ë°ì´í„° (ì‹¤ì œë¡œëŠ” OpenAI API ì‘ë‹µì—ì„œ ê°€ì ¸ì˜´)
            job_data = {
                "job_id": f"ft-{datetime.now().strftime('%Y%m%d%H%M%S')}",  # ì„ì‹œ ID
                "model": base_model,
                "status": "running",
                "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "finished_at": None,
                "fine_tuned_model": None,
                "training_file": "training_data.jsonl",
                "validation_file": None,
                "hyperparameters": {
                    "n_epochs": epochs,
                    "batch_size": batch_size,
                    "learning_rate_multiplier": learning_rate,
                },
                "result_files": [],
                "trained_tokens": None,
                "error": None,
                "updated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            }

            # ì‘ì—… ëª©ë¡ì— ì¶”ê°€
            if add_finetune_job(job_data):
                st.success("âœ… íŒŒì¸íŠœë‹ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤!")
                st.info(f"ì‘ì—… ID: `{job_data['job_id']}`")
                st.rerun()
            else:
                st.error("âŒ ì‘ì—… ì •ë³´ ì €ì¥ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")

    except Exception as e:
        st.error(f"âŒ íŒŒì¸íŠœë‹ ì‹œì‘ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


def log_model_deletion(model_id, success, message):
    """
    ëª¨ë¸ ì‚­ì œ ì‘ì—…ì„ ë¡œê·¸ì— ê¸°ë¡í•˜ëŠ” í•¨ìˆ˜

    Args:
        model_id (str): ëª¨ë¸ ID
        success (bool): ì‚­ì œ ì„±ê³µ ì—¬ë¶€
        message (str): ì‚­ì œ ê²°ê³¼ ë©”ì‹œì§€
    """
    try:
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "action": "model_deletion",
            "model_id": model_id,
            "success": success,
            "message": message,
            "user_session": st.session_state.get("session_id", "unknown"),
        }

        # ë¡œê·¸ íŒŒì¼ ê²½ë¡œ
        log_file = "logs/model_deletion.json"

        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs("logs", exist_ok=True)

        # ê¸°ì¡´ ë¡œê·¸ ë¡œë“œ
        logs = []
        if os.path.exists(log_file):
            try:
                with open(log_file, "r", encoding="utf-8") as f:
                    logs = json.load(f)
            except:
                logs = []  # íŒŒì¼ì´ ì†ìƒëœ ê²½ìš° ìƒˆë¡œ ì‹œì‘

        # ìƒˆ ë¡œê·¸ ì¶”ê°€
        logs.append(log_entry)

        # ë¡œê·¸ íŒŒì¼ ì €ì¥ (ìµœê·¼ 100ê°œë§Œ ìœ ì§€)
        with open(log_file, "w", encoding="utf-8") as f:
            json.dump(logs[-100:], f, ensure_ascii=False, indent=2)

    except Exception as e:
        st.warning(f"ë¡œê·¸ ê¸°ë¡ ì¤‘ ì˜¤ë¥˜: {str(e)}")


def remove_model_from_cache(model_id):
    """
    ë¡œì»¬ ìºì‹œì—ì„œ ì‚­ì œëœ ëª¨ë¸ ì •ë³´ë¥¼ ì œê±°í•˜ëŠ” í•¨ìˆ˜

    Args:
        model_id (str): ì œê±°í•  ëª¨ë¸ ID
    """
    try:
        # íŒŒì¸íŠœë‹ ì‘ì—… ìºì‹œì—ì„œ ì œê±°
        cache_files = [
            "finetune_jobs.json",
            "data/finetune_jobs.json",
            "cache/finetune_jobs.json",
        ]

        for cache_file in cache_files:
            if os.path.exists(cache_file):
                try:
                    with open(cache_file, "r", encoding="utf-8") as f:
                        jobs = json.load(f)

                    # í•´ë‹¹ ëª¨ë¸ì„ ê°€ì§„ ì‘ì—…ë“¤ì˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                    updated = False
                    for job in jobs:
                        if job.get("fine_tuned_model") == model_id:
                            job["model_deleted"] = True
                            job["deleted_at"] = datetime.now().isoformat()
                            updated = True

                    # ë³€ê²½ì‚¬í•­ì´ ìˆìœ¼ë©´ íŒŒì¼ ì €ì¥
                    if updated:
                        with open(cache_file, "w", encoding="utf-8") as f:
                            json.dump(jobs, f, ensure_ascii=False, indent=2)

                except Exception as e:
                    st.warning(f"ìºì‹œ íŒŒì¼ {cache_file} ì—…ë°ì´íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")

        # ì„¸ì…˜ ìƒíƒœì—ì„œë„ ì œê±°
        if "test_model" in st.session_state and st.session_state.test_model == model_id:
            del st.session_state.test_model

    except Exception as e:
        st.warning(f"ìºì‹œì—ì„œ ëª¨ë¸ ì •ë³´ ì œê±° ì¤‘ ì˜¤ë¥˜: {str(e)}")


def delete_finetuned_model(model_id):
    """
    OpenAI íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜

    Args:
        model_id (str): ì‚­ì œí•  ëª¨ë¸ì˜ ID

    Returns:
        tuple: (success: bool, message: str)
    """
    try:
        # OpenAI API í‚¤ í™•ì¸
        api_key = st.session_state.get("openai_api_key")
        if not api_key:
            return False, "OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        client = openai.OpenAI(api_key=api_key)

        # ëª¨ë¸ ID ìœ íš¨ì„± ê²€ì‚¬
        if not model_id or not isinstance(model_id, str):
            return False, "ìœ íš¨í•˜ì§€ ì•Šì€ ëª¨ë¸ IDì…ë‹ˆë‹¤."

        # ëª¨ë¸ì´ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì¸ì§€ í™•ì¸ (ë³´ì•ˆì„ ìœ„í•´)
        if not (model_id.startswith("ft:") or "ft-" in model_id):
            return False, "íŒŒì¸íŠœë‹ëœ ëª¨ë¸ë§Œ ì‚­ì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."

        # ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
        try:
            model_info = client.models.retrieve(model_id)
            if not model_info:
                return False, f"ëª¨ë¸ '{model_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        except Exception as e:
            if "404" in str(e) or "not found" in str(e).lower():
                return False, f"ëª¨ë¸ '{model_id}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
            else:
                return False, f"ëª¨ë¸ ì •ë³´ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}"

        # ëª¨ë¸ ì‚­ì œ ì‹¤í–‰
        try:
            delete_response = client.models.delete(model_id)

            # ì‚­ì œ ì‘ë‹µ í™•ì¸
            if hasattr(delete_response, "deleted") and delete_response.deleted:
                # ì‚­ì œ ë¡œê·¸ ê¸°ë¡
                log_model_deletion(model_id, True, "ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë¨")

                # ë¡œì»¬ ìºì‹œì—ì„œë„ í•´ë‹¹ ëª¨ë¸ ì •ë³´ ì œê±°
                remove_model_from_cache(model_id)

                return True, f"ëª¨ë¸ '{model_id}'ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
            else:
                log_model_deletion(model_id, False, "ì‚­ì œ ì‘ë‹µì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŒ")
                return False, f"ëª¨ë¸ ì‚­ì œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì‘ë‹µ: {delete_response}"

        except Exception as e:
            error_msg = str(e)

            # ì¼ë°˜ì ì¸ ì˜¤ë¥˜ ë©”ì‹œì§€ ì²˜ë¦¬
            if "insufficient permissions" in error_msg.lower():
                error_msg = "ëª¨ë¸ ì‚­ì œ ê¶Œí•œì´ ì—†ìŠµë‹ˆë‹¤. API í‚¤ ê¶Œí•œì„ í™•ì¸í•˜ì„¸ìš”."
            elif "model is currently being used" in error_msg.lower():
                error_msg = (
                    "ëª¨ë¸ì´ í˜„ì¬ ì‚¬ìš© ì¤‘ì…ë‹ˆë‹¤. ì‚¬ìš©ì„ ì¤‘ë‹¨í•œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."
                )
            elif "rate limit" in error_msg.lower():
                error_msg = "API ìš”ì²­ í•œë„ë¥¼ ì´ˆê³¼í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•˜ì„¸ìš”."

            log_model_deletion(model_id, False, error_msg)
            return False, f"ëª¨ë¸ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {error_msg}"

    except Exception as e:
        error_msg = f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        log_model_deletion(model_id, False, error_msg)
        return False, error_msg


def bulk_delete_models(model_ids):
    """
    ì—¬ëŸ¬ ëª¨ë¸ì„ ì¼ê´„ ì‚­ì œí•˜ëŠ” í•¨ìˆ˜

    Args:
        model_ids (list): ì‚­ì œí•  ëª¨ë¸ ID ëª©ë¡

    Returns:
        dict: ì‚­ì œ ê²°ê³¼ ìš”ì•½
    """
    results = {"success_count": 0, "fail_count": 0, "results": []}

    for model_id in model_ids:
        success, message = delete_finetuned_model(model_id)

        results["results"].append(
            {"model_id": model_id, "success": success, "message": message}
        )

        if success:
            results["success_count"] += 1
        else:
            results["fail_count"] += 1

    return results


def confirm_model_deletion_widget(model_id):
    """
    ëª¨ë¸ ì‚­ì œ í™•ì¸ì„ ìœ„í•œ Streamlit ìœ„ì ¯

    Args:
        model_id (str): ì‚­ì œí•  ëª¨ë¸ ID

    Returns:
        bool: ì‚­ì œ ì‹¤í–‰ ì—¬ë¶€
    """
    st.warning("âš ï¸ **ëª¨ë¸ ì‚­ì œ í™•ì¸**")
    st.write(f"ëª¨ë¸ ID: `{model_id}`")
    st.write("ì´ ì‘ì—…ì€ ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì •ë§ë¡œ ì‚­ì œí•˜ì‹œê² ìŠµë‹ˆê¹Œ?")

    # í™•ì¸ ì…ë ¥
    confirm_text = st.text_input(
        "í™•ì¸ì„ ìœ„í•´ ëª¨ë¸ IDë¥¼ ë‹¤ì‹œ ì…ë ¥í•˜ì„¸ìš”:", key=f"confirm_delete_{model_id}"
    )

    col1, col2 = st.columns(2)

    with col1:
        if st.button("âŒ ì·¨ì†Œ", key=f"cancel_delete_{model_id}"):
            return False

    with col2:
        if confirm_text == model_id:
            if st.button(
                "ğŸ—‘ï¸ ì‚­ì œ í™•ì¸", type="secondary", key=f"execute_delete_{model_id}"
            ):
                return True
        else:
            st.button("ğŸ—‘ï¸ ì‚­ì œ í™•ì¸", disabled=True, key=f"disabled_delete_{model_id}")

    return False


def get_deletion_logs():
    """
    ëª¨ë¸ ì‚­ì œ ë¡œê·¸ë¥¼ ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜

    Returns:
        list: ì‚­ì œ ë¡œê·¸ ëª©ë¡
    """
    try:
        log_file = "logs/model_deletion.json"
        if os.path.exists(log_file):
            with open(log_file, "r", encoding="utf-8") as f:
                return json.load(f)
        return []
    except Exception as e:
        st.error(f"ì‚­ì œ ë¡œê·¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return []


def test_finetuned_model(
    model_name: str, prompt: str, max_tokens: int = 150, temperature: float = 0.7
):
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    try:
        # í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ê°€ì ¸ì˜¤ê¸°
        api_key = os.getenv("OPENAI_API_KEY")

        if not api_key:
            raise Exception(
                "âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. .env íŒŒì¼ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            )

        # OpenAI í´ë¼ì´ì–¸íŠ¸ ìƒì„±
        client = openai.OpenAI(api_key=api_key)

        # ëª¨ë¸ ì‘ë‹µ ìƒì„±
        response = client.chat.completions.create(
            model=model_name,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            temperature=temperature,
        )

        return response.choices[0].message.content

    except openai.APIError as e:
        if "does not exist" in str(e):
            raise Exception(
                f"ëª¨ë¸ '{model_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ì´ë¦„ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
            )
        elif "insufficient_quota" in str(e):
            raise Exception("API ì‚¬ìš©ëŸ‰ì´ ë¶€ì¡±í•©ë‹ˆë‹¤. ê²°ì œ ì •ë³´ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
        else:
            raise Exception(f"OpenAI API ì˜¤ë¥˜: {str(e)}")
    except Exception as e:
        raise Exception(f"ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}")


with tab5:
    st.header("ì„¤ì •")

    # AI ëª¨ë¸ ì„¤ì •
    st.subheader("AI ëª¨ë¸ ì„¤ì •")

    col1, col2 = st.columns(2)

    with col1:
        # ì‚¬ìš©í•  ëª¨ë¸ ì„ íƒ
        available_models = [
            "gpt-4o",
            "gpt-4o-mini",
            "gpt-4-turbo",
            "gpt-4",
            "gpt-3.5-turbo",
        ]

        # âœ… íŒŒì¸íŠœë‹ëœ ëª¨ë¸ë“¤ ì¶”ê°€
        jobs = load_finetune_jobs()  # ì´ì œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°”ë¡œ ë°›ìŒ

        finetuned_models = []
        for job in jobs:
            if job.get("status") == "succeeded" and job.get("fine_tuned_model"):
                finetuned_models.append(job["fine_tuned_model"])

        all_models = available_models + finetuned_models

        selected_model = st.selectbox(
            "ì‚¬ìš©í•  ëª¨ë¸",
            options=all_models,
            index=(
                all_models.index(st.session_state.get("selected_model", "gpt-4o"))
                if st.session_state.get("selected_model", "gpt-4o") in all_models
                else 0
            ),
        )
        st.session_state.selected_model = selected_model

        if selected_model in finetuned_models:
            st.success("ğŸ¯ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤!")

    with col2:
        # ìë™ ë¶„ì„ ì„¤ì •
        auto_analysis = st.checkbox(
            "ìë™ ë¶„ì„ í™œì„±í™”",
            value=st.session_state.get("auto_analysis", False),
            help="ìƒˆ ì´ë¯¸ì§€ê°€ ë„ì°©í•˜ë©´ ìë™ìœ¼ë¡œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
        )
        st.session_state.auto_analysis = auto_analysis

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì„¤ì •
    st.subheader("ğŸ’­ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸")

    default_prompt = """ë‹¹ì‹ ì€ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•˜ì—¬ ë„ì¥ ë¶ˆëŸ‰ë¥ ì„ íŒë‹¨í•˜ëŠ” ì „ë¬¸ AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤. 

ë‹¤ìŒ ê¸°ì¤€ìœ¼ë¡œ íŒë‹¨í•´ì£¼ì„¸ìš”:
- ë¹¨ê°„ìƒ‰ ì¢…ì´ë¡œ ê°ì‹¸ì§„ ë¬¼ì²´: ë¶ˆëŸ‰í’ˆ
- íŒŒë€ìƒ‰ ì¢…ì´ë¡œ ê°ì‹¸ì§„ ë¬¼ì²´: ì •í’ˆ
- ê¸°íƒ€ ìƒ‰ìƒì´ë‚˜ íŠ¹ì´ì‚¬í•­ì´ ìˆëŠ” ê²½ìš°: ìƒì„¸íˆ ì„¤ëª…

íŒì • ê²°ê³¼ì™€ í•¨ê»˜ ì‹ ë¢°ë„(%)ë¥¼ ì œê³µí•´ì£¼ì„¸ìš”."""

    system_prompt = st.text_area(
        "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸",
        value=st.session_state.get("system_prompt", default_prompt),
        height=200,
        help="AIê°€ ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•  ë•Œ ì‚¬ìš©í•  ì§€ì¹¨ì„ ì…ë ¥í•˜ì„¸ìš”.",
    )
    st.session_state.system_prompt = system_prompt

    # íŒŒì¼ ê´€ë¦¬
    st.subheader("ğŸ“ íŒŒì¼ ê´€ë¦¬")

    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("ğŸ—‘ï¸ ì´ë¯¸ì§€ íŒŒì¼ ì •ë¦¬"):
            try:
                # 7ì¼ ì´ìƒ ëœ ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
                cutoff_date = datetime.now() - timedelta(days=7)
                deleted_count = 0

                for filename in os.listdir(UPLOAD_FOLDER):
                    filepath = os.path.join(UPLOAD_FOLDER, filename)
                    if os.path.isfile(filepath):
                        file_time = datetime.fromtimestamp(os.path.getctime(filepath))
                        if file_time < cutoff_date:
                            os.remove(filepath)
                            deleted_count += 1

                st.success(f"âœ… {deleted_count}ê°œì˜ ì˜¤ë˜ëœ ì´ë¯¸ì§€ íŒŒì¼ì„ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"âŒ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    with col2:
        if st.button("ğŸ“Š ë¡œê·¸ ë°±ì—…"):
            try:
                # ë¡œê·¸ íŒŒì¼ ë°±ì—…
                backup_name = f"analysis_log_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
                backup_path = os.path.join(DATA_LOG_FOLDER, backup_name)
                shutil.copy2(LOG_CSV_PATH, backup_path)
                st.success(f"âœ… ë¡œê·¸ê°€ ë°±ì—…ë˜ì—ˆìŠµë‹ˆë‹¤: {backup_name}")
            except Exception as e:
                st.error(f"âŒ ë¡œê·¸ ë°±ì—… ì‹¤íŒ¨: {e}")

    with col3:
        # í›ˆë ¨ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        if os.path.exists(TRAINING_DATA_PATH):
            with open(TRAINING_DATA_PATH, "r", encoding="utf-8") as f:
                training_data = f.read()

            st.download_button(
                label="ğŸ“¥ í›ˆë ¨ ë°ì´í„° ë‹¤ìš´ë¡œë“œ",
                data=training_data,
                file_name="training_data.jsonl",
                mime="application/json",
            )

    # ì‹œìŠ¤í…œ ì •ë³´
    st.subheader("ì‹œìŠ¤í…œ ì •ë³´")

    col1, col2 = st.columns(2)

    with col1:
        st.write("**ë””ë ‰í† ë¦¬ ì •ë³´:**")
        st.write(f"- ì´ë¯¸ì§€ ì €ì¥ì†Œ: {UPLOAD_FOLDER}")
        st.write(f"- ë¶„ì„ ë¡œê·¸: {DATA_LOG_FOLDER}")
        st.write(f"- íŒŒì¸íŠœë‹ ë°ì´í„°: {FINETUNE_FOLDER}")
        st.write(f"- íŒŒì¸íŠœë‹ ì‘ì—…: {FINETUNE_JOBS_FOLDER}")

    with col2:
        st.write("**íŒŒì¼ ê°œìˆ˜:**")
        try:
            image_count = len(
                [
                    f
                    for f in os.listdir(UPLOAD_FOLDER)
                    if f.lower().endswith((".png", ".jpg", ".jpeg"))
                ]
            )
            log_count = len(
                [f for f in os.listdir(DATA_LOG_FOLDER) if f.endswith(".csv")]
            )
            finetune_count = len(
                [f for f in os.listdir(FINETUNE_FOLDER) if f.endswith(".jsonl")]
            )

            st.write(f"- ì´ë¯¸ì§€ íŒŒì¼: {image_count}ê°œ")
            st.write(f"- ë¡œê·¸ íŒŒì¼: {log_count}ê°œ")
            st.write(f"- í›ˆë ¨ ë°ì´í„°: {finetune_count}ê°œ")

            # ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ì •ë³´
            total_size = 0
            for folder in [UPLOAD_FOLDER, DATA_LOG_FOLDER, FINETUNE_FOLDER]:
                for root, dirs, files in os.walk(folder):
                    for file in files:
                        total_size += os.path.getsize(os.path.join(root, file))

            total_size_mb = total_size / (1024 * 1024)
            st.write(f"- ì´ ì‚¬ìš©ëŸ‰: {total_size_mb:.2f} MB")

        except Exception as e:
            st.error(f"ì‹œìŠ¤í…œ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")

    # ê³ ê¸‰ ì„¤ì •
    st.subheader("ğŸ”§ ê³ ê¸‰ ì„¤ì •")

    col1, col2 = st.columns(2)

    with col1:
        # ì´ë¯¸ì§€ í’ˆì§ˆ ì„¤ì •
        image_quality = st.slider(
            "ì´ë¯¸ì§€ ì••ì¶• í’ˆì§ˆ",
            min_value=10,
            max_value=100,
            value=st.session_state.get("image_quality", 85),
            step=5,
            help="ì—…ë¡œë“œëœ ì´ë¯¸ì§€ì˜ ì••ì¶• í’ˆì§ˆì„ ì„¤ì •í•©ë‹ˆë‹¤. ë†’ì„ìˆ˜ë¡ ìš©ëŸ‰ì´ ì»¤ì§‘ë‹ˆë‹¤.",
        )
        st.session_state.image_quality = image_quality

        # ìµœëŒ€ ì´ë¯¸ì§€ í¬ê¸° ì„¤ì •
        max_image_size = st.number_input(
            "ìµœëŒ€ ì´ë¯¸ì§€ í¬ê¸° (MB)",
            min_value=1,
            max_value=50,
            value=st.session_state.get("max_image_size", 10),
            help="ì—…ë¡œë“œ ê°€ëŠ¥í•œ ìµœëŒ€ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.",
        )
        st.session_state.max_image_size = max_image_size

    with col2:
        # API ìš”ì²­ íƒ€ì„ì•„ì›ƒ ì„¤ì •
        api_timeout = st.number_input(
            "API íƒ€ì„ì•„ì›ƒ (ì´ˆ)",
            min_value=10,
            max_value=300,
            value=st.session_state.get("api_timeout", 60),
            help="OpenAI API ìš”ì²­ì˜ íƒ€ì„ì•„ì›ƒ ì‹œê°„ì„ ì„¤ì •í•©ë‹ˆë‹¤.",
        )
        st.session_state.api_timeout = api_timeout

        # ìµœëŒ€ ë¶„ì„ ê¸°ë¡ ìˆ˜
        max_records = st.number_input(
            "ìµœëŒ€ ë¶„ì„ ê¸°ë¡ ìˆ˜",
            min_value=100,
            max_value=10000,
            value=st.session_state.get("max_records", 1000),
            step=100,
            help="ì €ì¥í•  ìµœëŒ€ ë¶„ì„ ê¸°ë¡ ìˆ˜ì…ë‹ˆë‹¤. ì´ˆê³¼ ì‹œ ì˜¤ë˜ëœ ê¸°ë¡ì´ ì‚­ì œë©ë‹ˆë‹¤.",
        )
        st.session_state.max_records = max_records

    # ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬
    st.subheader("ğŸ—„ï¸ ë°ì´í„°ë² ì´ìŠ¤ ê´€ë¦¬")

    col1, col2, col3 = st.columns(3)

    with col1:
        if st.button("ğŸ”„ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™”"):
            try:
                # CSV íŒŒì¼ ìµœì í™” (ì¤‘ë³µ ì œê±°, ì •ë ¬)
                if os.path.exists(LOG_CSV_PATH):
                    df = pd.read_csv(LOG_CSV_PATH, encoding="utf-8")
                    # ì¤‘ë³µ ì œê±° (íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ì¤€)
                    df = df.drop_duplicates(subset=["timestamp"], keep="last")
                    # ìµœì‹  ìˆœìœ¼ë¡œ ì •ë ¬
                    df = df.sort_values("timestamp", ascending=False)
                    # ìµœëŒ€ ê¸°ë¡ ìˆ˜ ì œí•œ
                    df = df.head(st.session_state.get("max_records", 1000))
                    df.to_csv(LOG_CSV_PATH, index=False)
                    st.success("âœ… ë°ì´í„°ë² ì´ìŠ¤ê°€ ìµœì í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                else:
                    st.warning("âš ï¸ ë¶„ì„ ë¡œê·¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ìµœì í™” ì‹¤íŒ¨: {e}")

    with col2:
        if st.button("ğŸ“¤ ì „ì²´ ë°ì´í„° ë‚´ë³´ë‚´ê¸°"):
            try:
                # ëª¨ë“  ë°ì´í„°ë¥¼ ZIPìœ¼ë¡œ ì••ì¶•
                import zipfile

                zip_name = f"quality_control_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.zip"
                zip_path = os.path.join(DATA_LOG_FOLDER, zip_name)

                with zipfile.ZipFile(zip_path, "w") as zipf:
                    # ë¡œê·¸ íŒŒì¼ ì¶”ê°€
                    if os.path.exists(LOG_CSV_PATH):
                        zipf.write(LOG_CSV_PATH, "new_analysis_log.csv")

                    # í›ˆë ¨ ë°ì´í„° ì¶”ê°€
                    if os.path.exists(TRAINING_DATA_PATH):
                        zipf.write(TRAINING_DATA_PATH, "training_data.jsonl")

                    # ìµœê·¼ ì´ë¯¸ì§€ íŒŒì¼ë“¤ ì¶”ê°€ (ìµœëŒ€ 100ê°œ)
                    image_files = [
                        f
                        for f in os.listdir(UPLOAD_FOLDER)
                        if f.lower().endswith((".png", ".jpg", ".jpeg"))
                    ]
                    image_files.sort(
                        key=lambda x: os.path.getctime(os.path.join(UPLOAD_FOLDER, x)),
                        reverse=True,
                    )

                    for i, img_file in enumerate(image_files[:100]):
                        img_path = os.path.join(UPLOAD_FOLDER, img_file)
                        zipf.write(img_path, f"images/{img_file}")

                # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ì œê³µ
                with open(zip_path, "rb") as f:
                    st.download_button(
                        label="ğŸ“¥ ZIP íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                        data=f.read(),
                        file_name=zip_name,
                        mime="application/zip",
                    )

                # ì„ì‹œ ZIP íŒŒì¼ ì‚­ì œ
                os.remove(zip_path)

            except Exception as e:
                st.error(f"âŒ ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")

    with col3:
        if st.button("âš ï¸ ëª¨ë“  ë°ì´í„° ì´ˆê¸°í™”", type="secondary"):
            if st.button("ğŸ”´ ì •ë§ë¡œ ì´ˆê¸°í™”í•˜ì‹œê² ìŠµë‹ˆê¹Œ?", type="primary"):
                try:
                    # ëª¨ë“  íŒŒì¼ ì‚­ì œ
                    for folder in [UPLOAD_FOLDER, DATA_LOG_FOLDER, FINETUNE_FOLDER]:
                        for filename in os.listdir(folder):
                            file_path = os.path.join(folder, filename)
                            if os.path.isfile(file_path):
                                os.remove(file_path)

                    # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
                    for key in list(st.session_state.keys()):
                        if key not in [
                            "selected_model",
                            "system_prompt",
                            "auto_analysis",
                        ]:
                            del st.session_state[key]

                    st.success("âœ… ëª¨ë“  ë°ì´í„°ê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    st.rerun()

                except Exception as e:
                    st.error(f"âŒ ë°ì´í„° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

    # ì„¤ì • ì €ì¥/ë¡œë“œ
    st.subheader("ğŸ’¾ ì„¤ì • ê´€ë¦¬")

    col1, col2 = st.columns(2)

    with col1:
        if st.button("ğŸ’¾ í˜„ì¬ ì„¤ì • ì €ì¥"):
            try:
                settings = {
                    "selected_model": st.session_state.get("selected_model", "gpt-4o"),
                    "system_prompt": st.session_state.get(
                        "system_prompt", default_prompt
                    ),
                    "auto_analysis": st.session_state.get("auto_analysis", True),
                    "image_quality": st.session_state.get("image_quality", 85),
                    "max_image_size": st.session_state.get("max_image_size", 10),
                    "api_timeout": st.session_state.get("api_timeout", 60),
                    "max_records": st.session_state.get("max_records", 1000),
                }

                settings_path = os.path.join(DATA_LOG_FOLDER, "settings.json")
                with open(settings_path, "w", encoding="utf-8") as f:
                    json.dump(settings, f, ensure_ascii=False, indent=2)

                st.success("âœ… ì„¤ì •ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"âŒ ì„¤ì • ì €ì¥ ì‹¤íŒ¨: {e}")

    with col2:
        if st.button("ğŸ“‚ ì €ì¥ëœ ì„¤ì • ë¡œë“œ"):
            try:
                settings_path = os.path.join(DATA_LOG_FOLDER, "settings.json")
                if os.path.exists(settings_path):
                    with open(settings_path, "r", encoding="utf-8") as f:
                        settings = json.load(f)

                    # ì„¸ì…˜ ìƒíƒœì— ì„¤ì • ì ìš©
                    for key, value in settings.items():
                        st.session_state[key] = value

                    st.success("âœ… ì €ì¥ëœ ì„¤ì •ì´ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                    st.rerun()
                else:
                    st.warning("âš ï¸ ì €ì¥ëœ ì„¤ì • íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            except Exception as e:
                st.error(f"âŒ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")

    # ì•± ì •ë³´
    st.subheader("â„¹ï¸ ì• í”Œë¦¬ì¼€ì´ì…˜ ì •ë³´")

    info_col1, info_col2 = st.columns(2)

    with info_col1:
        st.write("**ë²„ì „ ì •ë³´:**")
        st.write("- ì•± ë²„ì „: v1.0.0")
        st.write("- Streamlit ë²„ì „:", st.__version__)
        st.write("- Python ë²„ì „:", sys.version.split()[0])

    with info_col2:
        st.write("**ê¸°ëŠ¥:**")
        st.write("- ğŸ–¼ï¸ ì´ë¯¸ì§€ í’ˆì§ˆ ë¶„ì„")
        st.write("- ğŸ¤– AI ëª¨ë¸ íŒŒì¸íŠœë‹")
        st.write("- ğŸ“Š ì‹¤ì‹œê°„ ëŒ€ì‹œë³´ë“œ")
        st.write("- ğŸ“ˆ ìƒì„¸ í†µê³„ ë¶„ì„")
        st.write("- âš™ï¸ ê³ ê¸‰ ì„¤ì • ê´€ë¦¬")


def display_job_details(job: Dict[str, Any]):
    """ì‘ì—… ìƒì„¸ ì •ë³´ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤."""
    try:
        # ê¸°ë³¸ ì •ë³´
        col1, col2 = st.columns(2)

        with col1:
            st.write(f"**ì‘ì—… ID:** `{job.get('job_id', 'N/A')}`")
            st.write(f"**ê¸°ë³¸ ëª¨ë¸:** {job.get('model', 'N/A')}")
            st.write(f"**ìƒì„±ì¼ì‹œ:** {job.get('created_at', 'N/A')}")

        with col2:
            st.write(f"**ìƒíƒœ:** {job.get('status', 'N/A')}")
            st.write(f"**ì™„ë£Œì¼ì‹œ:** {job.get('finished_at', 'N/A')}")
            st.write(f"**íŒŒì¸íŠœë‹ ëª¨ë¸:** {job.get('fine_tuned_model', 'N/A')}")

        # í•˜ì´í¼íŒŒë¼ë¯¸í„°
        hyperparams = job.get("hyperparameters", {})
        if hyperparams:
            st.write("**í•˜ì´í¼íŒŒë¼ë¯¸í„°:**")
            param_cols = st.columns(3)
            with param_cols[0]:
                st.write(f"- ì—í¬í¬: {hyperparams.get('n_epochs', 'N/A')}")
            with param_cols[1]:
                st.write(f"- ë°°ì¹˜ í¬ê¸°: {hyperparams.get('batch_size', 'N/A')}")
            with param_cols[2]:
                st.write(
                    f"- í•™ìŠµë¥ : {hyperparams.get('learning_rate_multiplier', 'N/A')}"
                )

        # í›ˆë ¨ ì •ë³´
        if job.get("trained_tokens"):
            st.write(f"**í›ˆë ¨ëœ í† í° ìˆ˜:** {job.get('trained_tokens'):,}")

        # ì—ëŸ¬ ì •ë³´
        error = job.get("error")
        if error:
            st.error(f"**ì˜¤ë¥˜:** {error.get('message', 'Unknown error')}")

        # íŒŒì¼ ì •ë³´
        files_info = []
        if job.get("training_file"):
            files_info.append(f"í›ˆë ¨ íŒŒì¼: {job.get('training_file')}")
        if job.get("validation_file"):
            files_info.append(f"ê²€ì¦ íŒŒì¼: {job.get('validation_file')}")

        if files_info:
            st.write("**íŒŒì¼ ì •ë³´:**")
            for info in files_info:
                st.write(f"- {info}")

        # ì‘ì—… ì¡°ì‘ ë²„íŠ¼ë“¤
        if job.get("status") == "running":
            col1, col2 = st.columns(2)
            with col1:
                if st.button(f"â¹ï¸ ì·¨ì†Œ", key=f"cancel_{job.get('job_id')}"):
                    cancel_job(job.get("job_id"))
            with col2:
                if st.button(f"ğŸ”„ ìƒíƒœ í™•ì¸", key=f"check_{job.get('job_id')}"):
                    check_job_status(job.get("job_id"))

        elif job.get("status") == "succeeded" and job.get("fine_tuned_model"):
            if st.button(f"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸", key=f"test_{job.get('job_id')}"):
                st.session_state.test_model_id = job.get("fine_tuned_model")
                st.info(f"í…ŒìŠ¤íŠ¸í•  ëª¨ë¸: {job.get('fine_tuned_model')}")

    except Exception as e:
        st.error(f"ì‘ì—… ì •ë³´ í‘œì‹œ ì¤‘ ì˜¤ë¥˜: {str(e)}")


with tab3:
    st.header("íŒŒì¸íŠœë‹ ê´€ë¦¬")

    # íŒŒì¸íŠœë‹ ì„¤ì •
    st.subheader("íŒŒì¸íŠœë‹ ì„¤ì •")

    col1, col2 = st.columns(2)

    with col1:
        finetune_epochs = st.number_input(
            "ì—í¬í¬ ìˆ˜",
            min_value=1,
            max_value=10,
            value=st.session_state.get("finetune_epochs", 3),
            help="í›ˆë ¨ ë°˜ë³µ íšŸìˆ˜ (1-10)",
        )
        st.session_state.finetune_epochs = finetune_epochs

        finetune_batch_size = st.selectbox(
            "ë°°ì¹˜ í¬ê¸°",
            options=[1, 2, 4, 8, 16],
            index=[1, 2, 4, 8, 16].index(
                st.session_state.get("finetune_batch_size", 1)
            ),
            help="ë°°ì¹˜ í¬ê¸°ê°€ í´ìˆ˜ë¡ ì•ˆì •ì ì´ì§€ë§Œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€",
        )
        st.session_state.finetune_batch_size = finetune_batch_size

    with col2:
        finetune_learning_rate = st.number_input(
            "í•™ìŠµë¥ ",
            min_value=0.00001,
            max_value=0.1,
            value=st.session_state.get("finetune_learning_rate", 0.0001),
            step=0.00001,
            format="%.5f",
            help="í•™ìŠµë¥  (ì¼ë°˜ì ìœ¼ë¡œ 0.0001-0.001 ì‚¬ì´)",
        )
        st.session_state.finetune_learning_rate = finetune_learning_rate

        base_model = st.selectbox(
            "ê¸°ë³¸ ëª¨ë¸",
            options=["gpt-3.5-turbo-1106", "gpt-4-turbo-preview", "gpt-4o-mini"],
            index=0,
            help="íŒŒì¸íŠœë‹í•  ê¸°ë³¸ ëª¨ë¸ ì„ íƒ",
        )

    # í›ˆë ¨ ë°ì´í„° ì¤€ë¹„
    st.subheader("ğŸ“Š í›ˆë ¨ ë°ì´í„° ì¤€ë¹„")

    # ë¶„ì„ ë¡œê·¸ ë°ì´í„° í™•ì¸
    try:
        df = load_analysis_logs()
        if not df.empty:
            feedback_count = len(
                df[df["user_feedback"].notna() & (df["user_feedback"] != "")]
            )

            # ë°ì´í„° í’ˆì§ˆ ì²´í¬
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ì´ ë°ì´í„°", len(df))
            with col2:
                st.metric("í”¼ë“œë°± ë°ì´í„°", feedback_count)
            with col3:
                if feedback_count >= 10:
                    st.metric("ìƒíƒœ", "ì¤€ë¹„ì™„ë£Œ", delta="âœ…")
                else:
                    st.metric("ìƒíƒœ", "ë¶€ì¡±", delta="âŒ")

            # ìµœì†Œ ìš”êµ¬ì‚¬í•­ ì²´í¬
            if feedback_count < 10:
                st.error(
                    "âš ï¸ íŒŒì¸íŠœë‹ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 10ê°œì˜ í”¼ë“œë°± ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤."
                )
                st.info(
                    "í˜„ì¬ OpenAI API ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ ìµœì†Œ 10ê°œì˜ í›ˆë ¨ ìƒ˜í”Œì´ í•„ìš”í•©ë‹ˆë‹¤."
                )
            elif feedback_count < 50:
                st.warning("ğŸ’¡ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ìœ„í•´ 50ê°œ ì´ìƒì˜ ë°ì´í„°ë¥¼ ê¶Œì¥í•©ë‹ˆë‹¤.")

            if feedback_count >= 10:
                # ë°ì´í„° ë¶„í•  ì˜µì…˜
                train_ratio = st.slider(
                    "í›ˆë ¨/ê²€ì¦ ë°ì´í„° ë¹„ìœ¨",
                    min_value=0.7,
                    max_value=0.95,
                    value=0.8,
                    step=0.05,
                    help="í›ˆë ¨ ë°ì´í„° ë¹„ìœ¨ (ë‚˜ë¨¸ì§€ëŠ” ê²€ì¦ ë°ì´í„°ë¡œ ì‚¬ìš©)",
                )

                if st.button("ğŸ“¦ í›ˆë ¨ ë°ì´í„° ì¤€ë¹„", type="primary"):
                    with st.spinner("í›ˆë ¨ ë°ì´í„°ë¥¼ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                        try:
                            sample_count, message = prepare_training_data()

                            if sample_count > 0:
                                st.success(f"âœ… {message}")

                                # ë°ì´í„° í†µê³„ í‘œì‹œ
                                train_count = int(sample_count * train_ratio)
                                val_count = sample_count - train_count

                                col1, col2 = st.columns(2)
                                with col1:
                                    st.info(f"ğŸ“š í›ˆë ¨ ë°ì´í„°: {train_count}ê°œ")
                                with col2:
                                    st.info(f"ğŸ” ê²€ì¦ ë°ì´í„°: {val_count}ê°œ")

                                # íŒŒì¸íŠœë‹ ì‹œì‘ ì„¹ì…˜
                                st.subheader("ğŸš€ íŒŒì¸íŠœë‹ ì‹œì‘")

                                # ë¹„ìš© ì¶”ì •
                                estimated_cost = estimate_finetuning_cost(
                                    sample_count, base_model
                                )
                                st.info(f"ğŸ’° ì˜ˆìƒ ë¹„ìš©: ${estimated_cost:.2f}")

                                # í™•ì¸ ì²´í¬ë°•ìŠ¤
                                confirm_start = st.checkbox(
                                    "ìœ„ ì„¤ì •ìœ¼ë¡œ íŒŒì¸íŠœë‹ì„ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤.",
                                    help="ì²´í¬ í›„ íŒŒì¸íŠœë‹ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
                                )

                                if confirm_start and st.button(
                                    "ğŸš€ íŒŒì¸íŠœë‹ ì‹œì‘", type="secondary"
                                ):
                                    start_finetuning_process(
                                        sample_count,
                                        base_model,
                                        finetune_epochs,
                                        finetune_batch_size,
                                        finetune_learning_rate,
                                    )
                            else:
                                st.error(f"âŒ {message}")
                        except Exception as e:
                            st.error(f"âŒ ë°ì´í„° ì¤€ë¹„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                            st.exception(e)  # ë””ë²„ê¹…ì„ ìœ„í•œ ìƒì„¸ ì˜¤ë¥˜ í‘œì‹œ
        else:
            st.info("ë¶„ì„ ë¡œê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € í…ìŠ¤íŠ¸ ë¶„ì„ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.")
    except Exception as e:
        st.error(f"âŒ ë¶„ì„ ë¡œê·¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        st.exception(e)

    # íŒŒì¸íŠœë‹ ì‘ì—… ìƒíƒœ
    st.subheader("ğŸ“‹ íŒŒì¸íŠœë‹ ì‘ì—… í˜„í™©")

    # ì‘ì—… ëª©ë¡ ë¡œë“œ ì‹œë„
    try:
        jobs = load_finetune_jobs()  # ì´ë¯¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤ê³  ê°€ì •

        # ì‘ì—…ì´ ìˆëŠ” ê²½ìš°
        if jobs:
            status_filter = st.selectbox(
                "ìƒíƒœ í•„í„°",
                options=["ì „ì²´", "running", "succeeded", "failed", "cancelled"],
                index=0,
            )

            filtered_jobs = (
                jobs
                if status_filter == "ì „ì²´"
                else [
                    job
                    for job in jobs
                    if isinstance(job, dict) and job.get("status") == status_filter
                ]
            )

            if filtered_jobs:
                st.info(f"ğŸ“Š ì´ {len(filtered_jobs)}ê°œì˜ ì‘ì—…ì´ ìˆìŠµë‹ˆë‹¤.")

                for job in reversed(filtered_jobs):
                    if not isinstance(job, dict):
                        continue  # ì•ˆì „í•˜ê²Œ ë¬´ì‹œ

                    status_color = {
                        "running": "ğŸŸ¡",
                        "succeeded": "ğŸŸ¢",
                        "failed": "ğŸ”´",
                        "cancelled": "âš«",
                    }.get(job.get("status"), "âšª")

                    job_id = job.get("job_id", "Unknown")
                    job_id_display = job_id[:12] + "..." if len(job_id) > 12 else job_id

                    with st.expander(
                        f"{status_color} {job_id_display} ({job.get('status', 'unknown')})",
                        expanded=False,
                    ):
                        display_job_details(job)
            else:
                st.info(f"'{status_filter}' ìƒíƒœì¸ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            st.info("ì•„ì§ íŒŒì¸íŠœë‹ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
            st.markdown(
                """
                **ì²˜ìŒ ì‚¬ìš©í•˜ì‹œë‚˜ìš”?**
                1. 'OpenAI íŒŒì¸íŠœë‹ ì‘ì—… ëª©ë¡' ì„¹ì…˜ì—ì„œ 'ğŸ“‹ ì „ì²´ ì‘ì—… ëª©ë¡ ì¡°íšŒ' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ê¸°ì¡´ ì‘ì—…ë“¤ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                2. ìœ„ì˜ í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì„¹ì…˜ì—ì„œ ìƒˆë¡œìš´ íŒŒì¸íŠœë‹ ì‘ì—…ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
                """
            )

    except Exception as e:
        st.error(f"âŒ íŒŒì¸íŠœë‹ ì‘ì—… ëª©ë¡ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        st.exception(e)

    # ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼
    col1, col2 = st.columns([1, 4])
    with col1:
        if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨", help="ì‘ì—… ëª©ë¡ì„ ìƒˆë¡œê³ ì¹¨í•©ë‹ˆë‹¤"):
            st.rerun()

    # OpenAI íŒŒì¸íŠœë‹ ì‘ì—… ëª©ë¡ ì„¹ì…˜
    st.subheader("ğŸ” OpenAI íŒŒì¸íŠœë‹ ì‘ì—… ëª©ë¡")

    col1, col2 = st.columns(2)

    with col1:
        if st.button("ğŸ“‹ ì „ì²´ ì‘ì—… ëª©ë¡ ì¡°íšŒ", type="primary"):
            with st.spinner("OpenAIì—ì„œ íŒŒì¸íŠœë‹ ì‘ì—… ëª©ë¡ì„ ì¡°íšŒí•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                try:
                    jobs_data = fetch_openai_jobs()
                    if jobs_data and jobs_data.get("jobs"):
                        job_list = jobs_data["jobs"]
                        st.success(f"âœ… {len(job_list)}ê°œì˜ ì‘ì—…ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

                        # ì‘ì—… ëª©ë¡ë§Œ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                        st.session_state.openai_jobs = job_list

                        # ë¡œì»¬ íŒŒì¼ì—ëŠ” ì‘ì—… ë¦¬ìŠ¤íŠ¸ë§Œ ì €ì¥
                        save_finetune_jobs(job_list)

                        st.rerun()
                    else:
                        st.info("ì¡°íšŒëœ íŒŒì¸íŠœë‹ ì‘ì—…ì´ ì—†ìŠµë‹ˆë‹¤.")
                except Exception as e:
                    st.error(f"âŒ ì‘ì—… ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
                    st.exception(e)

    with col2:
        if st.button("ğŸ—‘ï¸ ë¡œì»¬ ìºì‹œ ì‚­ì œ", help="ë¡œì»¬ì— ì €ì¥ëœ ì‘ì—… ëª©ë¡ì„ ì‚­ì œí•©ë‹ˆë‹¤"):
            try:
                clear_finetune_jobs_cache()
                st.success("âœ… ë¡œì»¬ ìºì‹œê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.")
                if "openai_jobs" in st.session_state:
                    del st.session_state.openai_jobs
                st.rerun()
            except Exception as e:
                st.error(f"âŒ ìºì‹œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    # íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ê´€ë¦¬
    st.subheader("íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ê´€ë¦¬")

    # ì„±ê³µí•œ ì‘ì—…ë“¤ì—ì„œ ëª¨ë¸ ëª©ë¡ ì¶”ì¶œ
    try:
        jobs_data = load_finetune_jobs()

        if isinstance(jobs_data, dict):
            jobs = jobs_data.get("jobs", [])
        elif isinstance(jobs_data, list):
            jobs = jobs_data
        else:
            jobs = []

        successful_jobs = [
            job
            for job in jobs
            if isinstance(job, dict)
            and job.get("status") == "succeeded"
            and job.get("fine_tuned_model")
        ]

        if successful_jobs:
            st.info(f"ğŸ“Š ì´ {len(successful_jobs)}ê°œì˜ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì´ ìˆìŠµë‹ˆë‹¤.")

            for job in successful_jobs:
                model_name = job.get("fine_tuned_model", "Unknown")
                model_display = (
                    model_name[:20] + "..." if len(model_name) > 20 else model_name
                )

                with st.expander(f"ğŸ¤– {model_display}", expanded=False):
                    col1, col2 = st.columns(2)

                    with col1:
                        st.write(f"**ëª¨ë¸ ID:** `{model_name}`")
                        st.write(f"**ê¸°ë³¸ ëª¨ë¸:** {job.get('model', 'Unknown')}")
                        st.write(f"**ì™„ë£Œ ì‹œê°„:** {job.get('finished_at', 'Unknown')}")
                        st.write(
                            f"**í›ˆë ¨ íŒŒì¼:** {job.get('training_file', 'Unknown')}"
                        )

                    with col2:
                        # ëª¨ë¸ í…ŒìŠ¤íŠ¸ ë²„íŠ¼
                        if st.button(f"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸", key=f"test_{model_name}"):
                            st.session_state.test_model = model_name
                            st.info(f"ëª¨ë¸ {model_name}ì´ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")

                        # ëª¨ë¸ ì‚­ì œ ë²„íŠ¼
                        if st.button(
                            f"ğŸ—‘ï¸ ëª¨ë¸ ì‚­ì œ", key=f"delete_{model_name}", type="secondary"
                        ):
                            st.warning(
                                "âš ï¸ ëª¨ë¸ ì‚­ì œëŠ” ë˜ëŒë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‹ ì¤‘í•˜ê²Œ ì§„í–‰í•˜ì„¸ìš”."
                            )
                            if st.button(
                                f"í™•ì¸: {model_name} ì‚­ì œ",
                                key=f"confirm_delete_{model_name}",
                            ):
                                try:
                                    delete_finetuned_model(model_name)
                                    st.success(
                                        f"âœ… ëª¨ë¸ {model_name}ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤."
                                    )
                                    st.rerun()
                                except Exception as e:
                                    st.error(f"âŒ ëª¨ë¸ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        else:
            st.info("ì•„ì§ ì™„ë£Œëœ íŒŒì¸íŠœë‹ ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")

    except Exception as e:
        st.error(f"âŒ ëª¨ë¸ ëª©ë¡ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")

    # ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì„¹ì…˜
    if st.session_state.get("test_model"):
        st.subheader("ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸")

        test_model = st.session_state.test_model
        st.info(f"í…ŒìŠ¤íŠ¸ ì¤‘ì¸ ëª¨ë¸: `{test_model}`")

        # í…ŒìŠ¤íŠ¸ ì…ë ¥
        test_prompt = st.text_area(
            "í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸",
            placeholder="íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•  í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ì„¸ìš”...",
            height=100,
        )

        col1, col2 = st.columns(2)

        with col1:
            if st.button("ğŸš€ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰", type="primary"):
                if test_prompt.strip():
                    with st.spinner("ëª¨ë¸ ì‘ë‹µì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                        try:
                            response = test_finetuned_model(test_model, test_prompt)
                            if response:
                                st.success("âœ… ëª¨ë¸ ì‘ë‹µ:")
                                st.write(response)
                        except Exception as e:
                            st.error(f"âŒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {str(e)}")
                else:
                    st.warning("í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")

        with col2:
            if st.button("âŒ í…ŒìŠ¤íŠ¸ ì¢…ë£Œ"):
                del st.session_state.test_model
                st.rerun()

    # ë„ì›€ë§ ì„¹ì…˜
    with st.expander("â“ íŒŒì¸íŠœë‹ ë„ì›€ë§", expanded=False):
        st.markdown(
            """
        ### íŒŒì¸íŠœë‹ í”„ë¡œì„¸ìŠ¤
        
        1. **ë°ì´í„° ì¤€ë¹„**: ìµœì†Œ 10ê°œì˜ í”¼ë“œë°± ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.
        2. **ì„¤ì • ì¡°ì •**: ì—í¬í¬ ìˆ˜, ë°°ì¹˜ í¬ê¸°, í•™ìŠµë¥ ì„ ì¡°ì •í•©ë‹ˆë‹¤.
        3. **í›ˆë ¨ ì‹œì‘**: ì„¤ì •ì„ í™•ì¸í•˜ê³  íŒŒì¸íŠœë‹ì„ ì‹œì‘í•©ë‹ˆë‹¤.
        4. **ëª¨ë‹ˆí„°ë§**: ì‘ì—… ìƒíƒœë¥¼ ì£¼ê¸°ì ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.
        5. **í…ŒìŠ¤íŠ¸**: ì™„ë£Œëœ ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•˜ì—¬ ì„±ëŠ¥ì„ í™•ì¸í•©ë‹ˆë‹¤.
        
        ### ê¶Œì¥ ì„¤ì •
        
        - **ì—í¬í¬ ìˆ˜**: 3-5ê°œ (ê³¼ì í•© ë°©ì§€)
        - **ë°°ì¹˜ í¬ê¸°**: ë°ì´í„° í¬ê¸°ì— ë”°ë¼ 1-8
        - **í•™ìŠµë¥ **: 0.0001-0.001 (ë³´í†µ 0.0001)
        
        ### ì£¼ì˜ì‚¬í•­
        
        - íŒŒì¸íŠœë‹ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤ (ìˆ˜ì‹­ ë¶„~ìˆ˜ ì‹œê°„)
        - ë¹„ìš©ì´ ë°œìƒí•˜ë¯€ë¡œ ì˜ˆìƒ ë¹„ìš©ì„ í™•ì¸í•˜ì„¸ìš”
        - í’ˆì§ˆ ì¢‹ì€ í›ˆë ¨ ë°ì´í„°ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤
        - ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ê²€ì¦ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”
        """
        )

        st.markdown("---")
        st.markdown(
            "**ë¬¸ì œê°€ ë°œìƒí•˜ë©´ OpenAI API ìƒíƒœë¥¼ í™•ì¸í•˜ê³ , ë¡œê·¸ë¥¼ ê²€í† í•˜ì„¸ìš”.**"
        )


def update_job_status(job_id):
    """OpenAI APIì—ì„œ íŠ¹ì • íŒŒì¸íŠœë‹ ì‘ì—… ìƒíƒœë¥¼ ê°±ì‹ í•˜ì—¬ ì €ì¥í•¨."""
    try:
        # OpenAIì—ì„œ íŒŒì¸íŠœë‹ ì‘ì—… ì •ë³´ ì¡°íšŒ
        response = openai.FineTune.retrieve(id=job_id)

        # ê¸°ì¡´ ì‘ì—… ëª©ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
        jobs = load_finetune_jobs()

        # ê°±ì‹ í•  ì‘ì—… ì •ë³´ ìƒì„±
        updated_job = {
            "job_id": response.id,
            "status": response.status,
            "created_at": response.created_at,
            "base_model": response.model,
            "fine_tuned_model": getattr(response, "fine_tuned_model", None),
            "training_samples": (
                response.training_files[0].get("bytes", "N/A")
                if response.training_files
                else "N/A"
            ),
            "hyperparameters": (
                response.hyperparameters if hasattr(response, "hyperparameters") else {}
            ),
            # í•„ìš”ì— ë”°ë¼ ë” í•„ë“œ ì¶”ê°€ ê°€ëŠ¥
        }

        # ê¸°ì¡´ jobs ëª©ë¡ì—ì„œ job_idê°€ ê°™ì€ í•­ëª©ì„ ì°¾ì•„ì„œ ê°±ì‹ í•˜ê±°ë‚˜ ì—†ìœ¼ë©´ ì¶”ê°€
        found = False
        for i, job in enumerate(jobs):
            if job.get("job_id") == job_id:
                jobs[i] = updated_job
                found = True
                break
        if not found:
            jobs.append(updated_job)

        # ì €ì¥
        save_finetune_jobs(jobs)
        st.success(f"ì‘ì—… {job_id} ìƒíƒœê°€ ì—…ë°ì´íŠ¸ ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        st.error(f"ì‘ì—… ìƒíƒœ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")


with tab4:
    st.header("í”¼ë“œë°± ê´€ë¦¬")

    # ê°•ì œ ìƒˆë¡œê³ ì¹¨ì„ ìœ„í•œ ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
    if "feedback_refresh_trigger" not in st.session_state:
        st.session_state.feedback_refresh_trigger = 0

    if "cached_df" not in st.session_state:
        st.session_state.cached_df = None

    if "last_refresh_time" not in st.session_state:
        st.session_state.last_refresh_time = 0

    # ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (ìºì‹± í¬í•¨)
    @st.cache_data(ttl=10)  # 10ì´ˆê°„ ìºì‹œ
    def load_analysis_logs_cached(refresh_trigger):
        try:
            if os.path.exists(LOG_CSV_PATH):
                df = pd.read_csv(LOG_CSV_PATH, encoding="utf-8")
                return df
            else:
                return pd.DataFrame()
        except Exception as e:
            st.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
            return pd.DataFrame()

    # ìˆ˜ë™ ìƒˆë¡œê³ ì¹¨ ë²„íŠ¼
    col_refresh1, col_refresh2, col_refresh3 = st.columns([1, 1, 4])

    with col_refresh1:
        if st.button("ğŸ”„ ìƒˆë¡œê³ ì¹¨", help="ë°ì´í„°ë¥¼ ê°•ì œë¡œ ìƒˆë¡œê³ ì¹¨í•©ë‹ˆë‹¤"):
            st.session_state.feedback_refresh_trigger += 1
            st.session_state.cached_df = None
            st.cache_data.clear()
            st.rerun()

    with col_refresh2:
        if st.button(
            "ğŸ—‘ï¸ ì „ì²´ ì‚­ì œ", help="ëª¨ë“  ë¶„ì„ ê¸°ë¡ì„ ì‚­ì œí•©ë‹ˆë‹¤", type="secondary"
        ):
            if st.session_state.get("confirm_delete_all", False):
                try:
                    if os.path.exists(LOG_CSV_PATH):
                        os.remove(LOG_CSV_PATH)
                    # ì—…ë¡œë“œëœ ì´ë¯¸ì§€ íŒŒì¼ë“¤ë„ ì‚­ì œ
                    if os.path.exists(UPLOAD_FOLDER):
                        for file in os.listdir(UPLOAD_FOLDER):
                            file_path = os.path.join(UPLOAD_FOLDER, file)
                            if os.path.isfile(file_path):
                                os.remove(file_path)

                    st.session_state.feedback_refresh_trigger += 1
                    st.session_state.cached_df = None
                    st.session_state.confirm_delete_all = False
                    st.cache_data.clear()
                    st.success("âœ… ëª¨ë“  ë°ì´í„°ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤!")
                    st.rerun()
                except Exception as e:
                    st.error(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
            else:
                st.session_state.confirm_delete_all = True
                st.warning("âš ï¸ ë‹¤ì‹œ í•œ ë²ˆ í´ë¦­í•˜ë©´ ëª¨ë“  ë°ì´í„°ê°€ ì‚­ì œë©ë‹ˆë‹¤!")

    # ë°ì´í„° ë¡œë“œ
    df = load_analysis_logs_cached(st.session_state.feedback_refresh_trigger)

    if not df.empty:
        # í”¼ë“œë°± í†µê³„
        st.subheader("ğŸ“Š í”¼ë“œë°± í˜„í™©")

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            total_analyses = len(df)
            st.metric("ì´ ë¶„ì„ ê±´ìˆ˜", total_analyses)

        with col2:
            feedback_count = len(
                df[df["user_feedback"].notna() & (df["user_feedback"] != "")]
            )
            st.metric("í”¼ë“œë°± ì™„ë£Œ", feedback_count)

        with col3:
            feedback_pending = total_analyses - feedback_count
            st.metric("í”¼ë“œë°± ëŒ€ê¸°", feedback_pending)

        with col4:
            if total_analyses > 0:
                feedback_rate = (feedback_count / total_analyses) * 100
                st.metric("í”¼ë“œë°± ì™„ë£Œìœ¨", f"{feedback_rate:.1f}%")

        # í”¼ë“œë°± ì§„í–‰ë¥  ì‹œê°í™”
        if total_analyses > 0:
            progress = feedback_count / total_analyses
            st.progress(progress, text=f"í”¼ë“œë°± ì§„í–‰ë¥ : {progress:.1%}")

        # í•„í„° ì˜µì…˜
        st.subheader("ğŸ” ë¶„ì„ ê²°ê³¼ í•„í„°ë§")

        col1, col2, col3 = st.columns(3)

        with col1:
            feedback_status = st.selectbox(
                "í”¼ë“œë°± ìƒíƒœ",
                options=["ì „ì²´", "í”¼ë“œë°± í•„ìš”", "í”¼ë“œë°± ì™„ë£Œ"],
                index=1,  # ê¸°ë³¸ê°’: í”¼ë“œë°± í•„ìš”
            )

        with col2:
            judgment_filter = st.selectbox(
                "íŒì • ê²°ê³¼",
                options=(
                    ["ì „ì²´"] + list(df["judgment"].unique())
                    if "judgment" in df.columns
                    else ["ì „ì²´"]
                ),
            )

        with col3:
            date_range = st.selectbox(
                "ê¸°ê°„", options=["ì „ì²´", "ì˜¤ëŠ˜", "ìµœê·¼ 3ì¼", "ìµœê·¼ 7ì¼", "ìµœê·¼ 30ì¼"]
            )

        # í•„í„° ì ìš©
        filtered_df = df.copy()

        # í”¼ë“œë°± ìƒíƒœ í•„í„°
        if feedback_status == "í”¼ë“œë°± í•„ìš”":
            filtered_df = filtered_df[
                filtered_df["user_feedback"].isna()
                | (filtered_df["user_feedback"] == "")
            ]
        elif feedback_status == "í”¼ë“œë°± ì™„ë£Œ":
            filtered_df = filtered_df[
                filtered_df["user_feedback"].notna()
                & (filtered_df["user_feedback"] != "")
            ]

        # íŒì • ê²°ê³¼ í•„í„°
        if judgment_filter != "ì „ì²´" and "judgment" in filtered_df.columns:
            filtered_df = filtered_df[filtered_df["judgment"] == judgment_filter]

        # ë‚ ì§œ í•„í„°
        if "timestamp" in filtered_df.columns and date_range != "ì „ì²´":
            now = datetime.now()
            if date_range == "ì˜¤ëŠ˜":
                cutoff = now.replace(hour=0, minute=0, second=0, microsecond=0)
            elif date_range == "ìµœê·¼ 3ì¼":
                cutoff = now - timedelta(days=3)
            elif date_range == "ìµœê·¼ 7ì¼":
                cutoff = now - timedelta(days=7)
            elif date_range == "ìµœê·¼ 30ì¼":
                cutoff = now - timedelta(days=30)

            filtered_df["timestamp"] = pd.to_datetime(filtered_df["timestamp"])
            filtered_df = filtered_df[filtered_df["timestamp"] >= cutoff]

        # ì •ë ¬ (ìµœì‹ ìˆœ)
        if "timestamp" in filtered_df.columns:
            filtered_df = filtered_df.sort_values("timestamp", ascending=False)

        # í”¼ë“œë°± ì…ë ¥ ì„¹ì…˜
        st.subheader("í”¼ë“œë°± ì…ë ¥")

        if not filtered_df.empty:
            st.write(f"**{len(filtered_df)}ê°œì˜ ê²°ê³¼ê°€ í•„í„° ì¡°ê±´ì— ë§ìŠµë‹ˆë‹¤**")

            # í˜ì´ì§€ë„¤ì´ì…˜
            items_per_page = 5
            total_pages = (len(filtered_df) - 1) // items_per_page + 1

            if total_pages > 1:
                page = st.selectbox(
                    "í˜ì´ì§€ ì„ íƒ",
                    range(1, total_pages + 1),
                    format_func=lambda x: f"{x} í˜ì´ì§€",
                )
                start_idx = (page - 1) * items_per_page
                end_idx = start_idx + items_per_page
                page_df = filtered_df.iloc[start_idx:end_idx]
            else:
                page_df = filtered_df.head(items_per_page)

            # ê° ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ í”¼ë“œë°± ì…ë ¥ í¼
            for idx, row in page_df.iterrows():
                with st.expander(
                    f"ğŸ“¸ {row.get('filename', 'Unknown')} - {row.get('judgment', 'Unknown')} "
                    f"({'í”¼ë“œë°± ì™„ë£Œ' if pd.notna(row.get('user_feedback')) and row.get('user_feedback') != '' else 'í”¼ë“œë°± í•„ìš”'})",
                    expanded=pd.isna(row.get("user_feedback"))
                    or row.get("user_feedback") == "",
                ):
                    delete_col1, delete_col2 = st.columns([6, 1])

                    with delete_col2:
                        # ì•ˆì •ì ì¸ key ìƒì„±
                        delete_key = f"delete_{idx}"
                        confirm_key = f"confirm_delete_{idx}"

                        if st.button(
                            "ğŸ—‘ï¸ ì‚­ì œ",
                            key=delete_key,
                            help="ì´ ê¸°ë¡ì„ ì‚­ì œí•©ë‹ˆë‹¤",
                            type="secondary",
                        ):
                            if st.session_state.get(confirm_key, False):
                                # ì‹¤ì œ ì‚­ì œ ìˆ˜í–‰
                                try:
                                    # CSVì—ì„œ í•´ë‹¹ í–‰ ì‚­ì œ
                                    df_update = pd.read_csv(LOG_CSV_PATH)

                                    # timestampë¡œ ë§¤ì¹­í•˜ì—¬ ì‚­ì œ
                                    if "timestamp" in df_update.columns:
                                        mask = (
                                            df_update["timestamp"] == row["timestamp"]
                                        )
                                        df_update = df_update[~mask]

                                        # ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
                                        img_path = None
                                        if "filepath" in row and pd.notna(
                                            row["filepath"]
                                        ):
                                            img_path = row["filepath"]
                                        elif "filename" in row:
                                            possible_path = os.path.join(
                                                UPLOAD_FOLDER, row["filename"]
                                            )
                                            if os.path.exists(possible_path):
                                                img_path = possible_path

                                        if img_path and os.path.exists(img_path):
                                            os.remove(img_path)

                                        # CSV ì €ì¥
                                        df_update.to_csv(LOG_CSV_PATH, index=False)

                                        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™” ë° ìƒˆë¡œê³ ì¹¨
                                        st.session_state.feedback_refresh_trigger += 1
                                        st.session_state.cached_df = None
                                        if confirm_key in st.session_state:
                                            del st.session_state[confirm_key]
                                        st.cache_data.clear()

                                        st.success("âœ… ê¸°ë¡ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤!")
                                        st.rerun()
                                    else:
                                        st.error(
                                            "âŒ íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë³´ê°€ ì—†ì–´ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                                        )

                                except Exception as e:
                                    st.error(f"âŒ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
                            else:
                                st.session_state[confirm_key] = True
                                st.warning("âš ï¸ ë‹¤ì‹œ í•œ ë²ˆ í´ë¦­í•˜ë©´ ì‚­ì œë©ë‹ˆë‹¤!")

                    col1, col2 = st.columns([2, 1])

                    with col1:
                        # ë¶„ì„ ì •ë³´ í‘œì‹œ
                        st.write("**ë¶„ì„ ì •ë³´:**")
                        st.write(f"- íŒŒì¼ëª…: {row.get('filename', 'Unknown')}")
                        st.write(f"- ë¶„ì„ ì‹œê°„: {row.get('timestamp', 'Unknown')}")
                        st.write(f"- AI íŒì •: {row.get('judgment', 'Unknown')}")
                        st.write(f"- ì‹ ë¢°ë„: {row.get('confidence_score', 'Unknown')}")
                        st.write(
                            f"- ì²˜ë¦¬ ì‹œê°„: {row.get('processing_time_seconds', 'Unknown')}ì´ˆ"
                        )
                        st.write(f"- ì‚¬ìš© ëª¨ë¸: {row.get('model_used', 'Unknown')}")

                        # ìƒì„¸ ë¶„ì„ ê²°ê³¼ í‘œì‹œ
                        if "analysis_details" in row and pd.notna(
                            row["analysis_details"]
                        ):
                            st.write("**ìƒì„¸ ë¶„ì„ ê²°ê³¼:**")
                            st.write(row["analysis_details"])

                    with col2:
                        # ì´ë¯¸ì§€ í‘œì‹œ (ê°€ëŠ¥í•œ ê²½ìš°)
                        img_path = None
                        if "filepath" in row and pd.notna(row["filepath"]):
                            img_path = row["filepath"]
                        elif "filename" in row:
                            # UPLOAD_FOLDERì—ì„œ ì´ë¯¸ì§€ ì°¾ê¸°
                            possible_path = os.path.join(UPLOAD_FOLDER, row["filename"])
                            if os.path.exists(possible_path):
                                img_path = possible_path

                        if img_path and os.path.exists(img_path):
                            try:
                                st.image(img_path, caption="ë¶„ì„ëœ ì´ë¯¸ì§€", width=200)
                            except Exception as e:
                                st.write("ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨")

                    # í”¼ë“œë°± ì…ë ¥ í¼
                    st.write("---")
                    st.write("**í”¼ë“œë°± ì…ë ¥:**")

                    feedback_col1, feedback_col2 = st.columns(2)

                    with feedback_col1:
                        # ì •í™•ì„± í‰ê°€
                        accuracy_options = [
                            "ì„ íƒí•˜ì„¸ìš”",
                            "ì •í™•í•¨ âœ…",
                            "ë¶€ì •í™•í•¨ âŒ",
                            "ë¶€ë¶„ì ìœ¼ë¡œ ì •í™•í•¨ âš ï¸",
                        ]

                        current_accuracy = "ì„ íƒí•˜ì„¸ìš”"
                        if (
                            pd.notna(row.get("user_feedback"))
                            and row.get("user_feedback") != ""
                        ):
                            for option in accuracy_options[1:]:
                                if option.split()[0] in str(
                                    row.get("user_feedback", "")
                                ):
                                    current_accuracy = option
                                    break

                        accuracy_feedback = st.selectbox(
                            "AI íŒì •ì˜ ì •í™•ì„±",
                            options=accuracy_options,
                            index=accuracy_options.index(current_accuracy),
                            key=f"accuracy_{idx}_{st.session_state.feedback_refresh_trigger}",
                        )

                    with feedback_col2:
                        # ì˜¬ë°”ë¥¸ íŒì • (AIê°€ í‹€ë¦° ê²½ìš°)
                        correct_judgment_options = ["í•´ë‹¹ì—†ìŒ", "ì •í’ˆ", "ë¶ˆëŸ‰", "ê¸°íƒ€"]

                        correct_judgment = st.selectbox(
                            "ì˜¬ë°”ë¥¸ íŒì • (AIê°€ í‹€ë¦° ê²½ìš°)",
                            options=correct_judgment_options,
                            key=f"correct_{idx}_{st.session_state.feedback_refresh_trigger}",
                        )

                    # ì¶”ê°€ ì½”ë©˜íŠ¸
                    current_comment = ""
                    if (
                        pd.notna(row.get("user_feedback"))
                        and row.get("user_feedback") != ""
                    ):
                        # ê¸°ì¡´ í”¼ë“œë°±ì—ì„œ ì½”ë©˜íŠ¸ ë¶€ë¶„ ì¶”ì¶œ
                        feedback_parts = str(row.get("user_feedback", "")).split("\n")
                        for part in feedback_parts:
                            if "ì½”ë©˜íŠ¸:" in part:
                                current_comment = part.replace("ì½”ë©˜íŠ¸:", "").strip()
                                break

                    additional_comment = st.text_area(
                        "ì¶”ê°€ ì½”ë©˜íŠ¸",
                        value=current_comment,
                        placeholder="ë¶„ì„ ê²°ê³¼ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ì˜ê²¬ì´ë‚˜ ê°œì„ ì‚¬í•­ì„ ì…ë ¥í•˜ì„¸ìš”...",
                        key=f"comment_{idx}_{st.session_state.feedback_refresh_trigger}",
                        height=100,
                    )

                    # í”¼ë“œë°± ì €ì¥ ë²„íŠ¼
                    if st.button(
                        f"ğŸ’¾ í”¼ë“œë°± ì €ì¥",
                        key=f"save_{idx}_{st.session_state.feedback_refresh_trigger}",
                    ):
                        if accuracy_feedback != "ì„ íƒí•˜ì„¸ìš”":
                            # í”¼ë“œë°± ë‚´ìš© êµ¬ì„±
                            feedback_content = f"ì •í™•ì„±: {accuracy_feedback}"

                            if correct_judgment != "í•´ë‹¹ì—†ìŒ":
                                feedback_content += f"\nì˜¬ë°”ë¥¸ íŒì •: {correct_judgment}"

                            if additional_comment.strip():
                                feedback_content += (
                                    f"\nì½”ë©˜íŠ¸: {additional_comment.strip()}"
                                )

                            # CSV íŒŒì¼ ì—…ë°ì´íŠ¸
                            try:
                                df_update = pd.read_csv(LOG_CSV_PATH)

                                # í•´ë‹¹ í–‰ ì°¾ê¸° (timestampë¡œ ë§¤ì¹­)
                                if "timestamp" in df_update.columns:
                                    mask = df_update["timestamp"] == row["timestamp"]
                                    if mask.any():
                                        df_update.loc[mask, "user_feedback"] = (
                                            feedback_content
                                        )
                                        df_update.loc[mask, "feedback_timestamp"] = (
                                            datetime.now().isoformat()
                                        )

                                        # ì˜¬ë°”ë¥¸ íŒì •ì´ ì œê³µëœ ê²½ìš°
                                        if correct_judgment != "í•´ë‹¹ì—†ìŒ":
                                            df_update.loc[mask, "correct_judgment"] = (
                                                correct_judgment
                                            )

                                        df_update.to_csv(LOG_CSV_PATH, index=False)

                                        # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                                        st.session_state.feedback_refresh_trigger += 1
                                        st.session_state.cached_df = None
                                        st.cache_data.clear()

                                        st.success("âœ… í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
                                        st.rerun()
                                    else:
                                        st.error(
                                            "âŒ í•´ë‹¹ ë¶„ì„ ê¸°ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                                        )
                                else:
                                    st.error("âŒ íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")

                            except Exception as e:
                                st.error(f"âŒ í”¼ë“œë°± ì €ì¥ ì‹¤íŒ¨: {str(e)}")
                        else:
                            st.warning("âš ï¸ ì •í™•ì„± í‰ê°€ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.")

            # ëŒ€ëŸ‰ í”¼ë“œë°± ì²˜ë¦¬
            st.subheader("âš¡ ëŒ€ëŸ‰ í”¼ë“œë°± ì²˜ë¦¬")

            col1, col2, col3 = st.columns(3)

            with col1:
                if st.button("âœ… í˜„ì¬ í˜ì´ì§€ ëª¨ë‘ 'ì •í™•í•¨'ìœ¼ë¡œ í‘œì‹œ"):
                    try:
                        df_update = pd.read_csv(LOG_CSV_PATH)
                        updated_count = 0

                        for idx, row in page_df.iterrows():
                            # í”¼ë“œë°±ì´ ì—†ëŠ” ê²½ìš°ë§Œ ì—…ë°ì´íŠ¸
                            if (
                                pd.isna(row.get("user_feedback"))
                                or row.get("user_feedback") == ""
                            ):
                                mask = df_update["timestamp"] == row["timestamp"]
                                if mask.any():
                                    df_update.loc[mask, "user_feedback"] = (
                                        "ì •í™•ì„±: ì •í™•í•¨ âœ…"
                                    )
                                    df_update.loc[mask, "feedback_timestamp"] = (
                                        datetime.now().isoformat()
                                    )
                                    updated_count += 1

                        if updated_count > 0:
                            df_update.to_csv(LOG_CSV_PATH, index=False)

                            # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                            st.session_state.feedback_refresh_trigger += 1
                            st.session_state.cached_df = None
                            st.cache_data.clear()

                            st.success(
                                f"âœ… {updated_count}ê°œ í•­ëª©ì˜ í”¼ë“œë°±ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!"
                            )
                            st.rerun()
                        else:
                            st.info("â„¹ï¸ ì—…ë°ì´íŠ¸í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")

                    except Exception as e:
                        st.error(f"âŒ ëŒ€ëŸ‰ í”¼ë“œë°± ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")

            with col2:
                if st.button("ğŸ”„ í˜„ì¬ í˜ì´ì§€ í”¼ë“œë°± ì´ˆê¸°í™”"):
                    try:
                        df_update = pd.read_csv(LOG_CSV_PATH)
                        updated_count = 0

                        for idx, row in page_df.iterrows():
                            mask = df_update["timestamp"] == row["timestamp"]
                            if mask.any():
                                df_update.loc[mask, "user_feedback"] = ""
                                df_update.loc[mask, "feedback_timestamp"] = ""
                                if "correct_judgment" in df_update.columns:
                                    df_update.loc[mask, "correct_judgment"] = ""
                                updated_count += 1

                        if updated_count > 0:
                            df_update.to_csv(LOG_CSV_PATH, index=False)

                            # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
                            st.session_state.feedback_refresh_trigger += 1
                            st.session_state.cached_df = None
                            st.cache_data.clear()

                            st.success(
                                f"âœ… {updated_count}ê°œ í•­ëª©ì˜ í”¼ë“œë°±ì´ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤!"
                            )
                            st.rerun()

                    except Exception as e:
                        st.error(f"âŒ í”¼ë“œë°± ì´ˆê¸°í™” ì‹¤íŒ¨: {str(e)}")

            with col3:
                if st.button("ğŸ—‘ï¸ í˜„ì¬ í˜ì´ì§€ ëª¨ë‘ ì‚­ì œ"):
                    confirm_batch_delete_key = "confirm_batch_delete"

                    if st.session_state.get(confirm_batch_delete_key, False):
                        try:
                            df_update = pd.read_csv(LOG_CSV_PATH)
                            deleted_count = 0

                            for idx, row in page_df.iterrows():
                                # CSVì—ì„œ ì‚­ì œ
                                mask = df_update["timestamp"] == row["timestamp"]
                                df_update = df_update[~mask]

                                # ì´ë¯¸ì§€ íŒŒì¼ ì‚­ì œ
                                img_path = None
                                if "filepath" in row and pd.notna(row["filepath"]):
                                    img_path = row["filepath"]
                                elif "filename" in row:
                                    possible_path = os.path.join(
                                        UPLOAD_FOLDER, row["filename"]
                                    )
                                    if os.path.exists(possible_path):
                                        img_path = possible_path

                                if img_path and os.path.exists(img_path):
                                    os.remove(img_path)

                                deleted_count += 1

                            if deleted_count > 0:
                                df_update.to_csv(LOG_CSV_PATH, index=False)

                                # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
                                st.session_state.feedback_refresh_trigger += 1
                                st.session_state.cached_df = None
                                st.session_state[confirm_batch_delete_key] = False
                                st.cache_data.clear()

                                st.success(
                                    f"âœ… {deleted_count}ê°œ í•­ëª©ì´ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤!"
                                )
                                st.rerun()

                        except Exception as e:
                            st.error(f"âŒ ëŒ€ëŸ‰ ì‚­ì œ ì‹¤íŒ¨: {str(e)}")
                    else:
                        st.session_state[confirm_batch_delete_key] = True
                        st.warning(
                            "âš ï¸ ë‹¤ì‹œ í•œ ë²ˆ í´ë¦­í•˜ë©´ í˜„ì¬ í˜ì´ì§€ì˜ ëª¨ë“  í•­ëª©ì´ ì‚­ì œë©ë‹ˆë‹¤!"
                        )

        else:
            st.info("ğŸ“ ì„ íƒí•œ í•„í„° ì¡°ê±´ì— ë§ëŠ” ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # í”¼ë“œë°± í†µê³„ ë° ë¶„ì„
        st.subheader("ğŸ“ˆ í”¼ë“œë°± ë¶„ì„")

        if feedback_count > 0:
            feedback_df = df[df["user_feedback"].notna() & (df["user_feedback"] != "")]

            col1, col2 = st.columns(2)

            with col1:
                # ì •í™•ì„± ë¶„í¬
                accuracy_counts = {"ì •í™•í•¨": 0, "ë¶€ì •í™•í•¨": 0, "ë¶€ë¶„ì ìœ¼ë¡œ ì •í™•í•¨": 0}

                for feedback in feedback_df["user_feedback"]:
                    if "ì •í™•í•¨ âœ…" in str(feedback):
                        accuracy_counts["ì •í™•í•¨"] += 1
                    elif "ë¶€ì •í™•í•¨ âŒ" in str(feedback):
                        accuracy_counts["ë¶€ì •í™•í•¨"] += 1
                    elif "ë¶€ë¶„ì ìœ¼ë¡œ ì •í™•í•¨ âš ï¸" in str(feedback):
                        accuracy_counts["ë¶€ë¶„ì ìœ¼ë¡œ ì •í™•í•¨"] += 1

                if sum(accuracy_counts.values()) > 0:
                    fig_accuracy = px.pie(
                        values=list(accuracy_counts.values()),
                        names=list(accuracy_counts.keys()),
                        title="AI íŒì • ì •í™•ì„± ë¶„í¬",
                        color_discrete_map={
                            "ì •í™•í•¨": "#28a745",
                            "ë¶€ì •í™•í•¨": "#dc3545",
                            "ë¶€ë¶„ì ìœ¼ë¡œ ì •í™•í•¨": "#ffc107",
                        },
                    )
                    st.plotly_chart(fig_accuracy, use_container_width=True)

            with col2:
                # ëª¨ë¸ë³„ ì •í™•ì„±
                if "model_used" in feedback_df.columns:
                    model_accuracy = {}

                    for idx, row in feedback_df.iterrows():
                        model = row.get("model_used", "Unknown")
                        feedback = str(row.get("user_feedback", ""))

                        if model not in model_accuracy:
                            model_accuracy[model] = {
                                "ì •í™•í•¨": 0,
                                "ë¶€ì •í™•í•¨": 0,
                                "ë¶€ë¶„ì ": 0,
                                "ì´í•©": 0,
                            }

                        model_accuracy[model]["ì´í•©"] += 1

                        if "ì •í™•í•¨ âœ…" in feedback:
                            model_accuracy[model]["ì •í™•í•¨"] += 1
                        elif "ë¶€ì •í™•í•¨ âŒ" in feedback:
                            model_accuracy[model]["ë¶€ì •í™•í•¨"] += 1
                        elif "ë¶€ë¶„ì ìœ¼ë¡œ ì •í™•í•¨ âš ï¸" in feedback:
                            model_accuracy[model]["ë¶€ë¶„ì "] += 1

                    # ì •í™•ë„ ê³„ì‚° ë° í‘œì‹œ
                    accuracy_data = []
                    for model, counts in model_accuracy.items():
                        if counts["ì´í•©"] > 0:
                            accuracy_rate = (counts["ì •í™•í•¨"] / counts["ì´í•©"]) * 100
                            accuracy_data.append(
                                {
                                    "ëª¨ë¸": model,
                                    "ì •í™•ë„": accuracy_rate,
                                    "ì´ í”¼ë“œë°±": counts["ì´í•©"],
                                }
                            )

                    if accuracy_data:
                        accuracy_df_display = pd.DataFrame(accuracy_data)
                        fig_model = px.bar(
                            accuracy_df_display,
                            x="ëª¨ë¸",
                            y="ì •í™•ë„",
                            title="ëª¨ë¸ë³„ ì •í™•ë„",
                            text="ì´ í”¼ë“œë°±",
                        )
                        fig_model.update_traces(textposition="outside")
                        fig_model.update_layout(
                            yaxis_title="ì •í™•ë„ (%)", xaxis_title="ëª¨ë¸"
                        )
                        st.plotly_chart(fig_model, use_container_width=True)

            # ê°œì„ ì‚¬í•­ ìš”ì•½
            st.subheader("ğŸ’¡ ê°œì„ ì‚¬í•­ ìš”ì•½")

            # ë¶€ì •í™•í•œ íŒì •ë“¤ ë¶„ì„
            incorrect_df = feedback_df[
                feedback_df["user_feedback"].str.contains("ë¶€ì •í™•í•¨", na=False)
            ]

            if not incorrect_df.empty:
                st.write(f"**ë¶€ì •í™•í•œ íŒì • {len(incorrect_df)}ê±´ ë°œê²¬:**")

                # ì˜¬ë°”ë¥¸ íŒì • ë¶„í¬ (ì‚¬ìš©ìê°€ ì œê³µí•œ ê²½ìš°)
                if "correct_judgment" in incorrect_df.columns:
                    correct_judgments = incorrect_df["correct_judgment"].value_counts()
                    if not correct_judgments.empty:
                        st.write("ì‚¬ìš©ìê°€ ì œê³µí•œ ì˜¬ë°”ë¥¸ íŒì •:")
                        for judgment, count in correct_judgments.items():
                            if pd.notna(judgment) and judgment != "":
                                st.write(f"- {judgment}: {count}ê±´")

                # ìì£¼ ì–¸ê¸‰ë˜ëŠ” ì½”ë©˜íŠ¸ í‚¤ì›Œë“œ
                all_comments = []
                for feedback in incorrect_df["user_feedback"]:
                    if "ì½”ë©˜íŠ¸:" in str(feedback):
                        comment_part = str(feedback).split("ì½”ë©˜íŠ¸:")[-1].strip()
                        all_comments.append(comment_part)

                if all_comments:
                    st.write("**ì£¼ìš” ê°œì„  í”¼ë“œë°±:**")
                    # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë¶„ì„ (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ê°€ëŠ¥)
                    from collections import Counter
                    import re

                    # ì½”ë©˜íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
                    all_words = []
                    for comment in all_comments:
                        # í•œê¸€, ì˜ë¬¸, ìˆ«ìë§Œ ì¶”ì¶œí•˜ê³  2ê¸€ì ì´ìƒì¸ ë‹¨ì–´ë§Œ
                        words = re.findall(r"[ê°€-í£a-zA-Z0-9]{2,}", comment.lower())
                        all_words.extend(words)

                    if all_words:
                        word_counts = Counter(all_words)
                        # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ í‘œì‹œ
                        top_keywords = word_counts.most_common(10)

                        for word, count in top_keywords:
                            if count > 1:  # 2ë²ˆ ì´ìƒ ì–¸ê¸‰ëœ í‚¤ì›Œë“œë§Œ
                                st.write(f"- '{word}': {count}íšŒ ì–¸ê¸‰")

                    # ì „ì²´ ì½”ë©˜íŠ¸ í‘œì‹œ (ì ‘ê¸° ê°€ëŠ¥)
                    with st.expander("ì „ì²´ ê°œì„  ì½”ë©˜íŠ¸ ë³´ê¸°"):
                        for i, comment in enumerate(all_comments, 1):
                            st.write(f"{i}. {comment}")

            else:
                st.info("ğŸ‰ ëª¨ë“  í”¼ë“œë°±ì´ ê¸ì •ì ì…ë‹ˆë‹¤!")

        # ë°ì´í„° ë‚´ë³´ë‚´ê¸° ì„¹ì…˜
        st.subheader("ğŸ’¾ ë°ì´í„° ë‚´ë³´ë‚´ê¸°")

        col1, col2, col3 = st.columns(3)

        with col1:
            # CSV ë‹¤ìš´ë¡œë“œ
            if st.button("ğŸ“¥ ì „ì²´ ë°ì´í„° CSV ë‹¤ìš´ë¡œë“œ"):
                try:
                    csv_data = df.to_csv(index=False, encoding="utf-8-sig")
                    st.download_button(
                        label="ğŸ“„ CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                        data=csv_data,
                        file_name=f"analysis_feedback_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv",
                    )
                except Exception as e:
                    st.error(f"âŒ CSV ìƒì„± ì‹¤íŒ¨: {str(e)}")

        with col2:
            # í”¼ë“œë°±ë§Œ CSV ë‹¤ìš´ë¡œë“œ
            if feedback_count > 0:
                if st.button("ğŸ“¥ í”¼ë“œë°± ë°ì´í„°ë§Œ CSV ë‹¤ìš´ë¡œë“œ"):
                    try:
                        feedback_only_df = df[
                            df["user_feedback"].notna() & (df["user_feedback"] != "")
                        ]
                        csv_data = feedback_only_df.to_csv(
                            index=False, encoding="utf-8-sig"
                        )
                        st.download_button(
                            label="ğŸ“„ í”¼ë“œë°± CSV íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                            data=csv_data,
                            file_name=f"feedback_only_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                            mime="text/csv",
                        )
                    except Exception as e:
                        st.error(f"âŒ í”¼ë“œë°± CSV ìƒì„± ì‹¤íŒ¨: {str(e)}")

        with col3:
            # í†µê³„ ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ
            if st.button("ğŸ“Š í†µê³„ ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ"):
                try:
                    # í†µê³„ ë¦¬í¬íŠ¸ ìƒì„±
                    report_lines = []
                    report_lines.append("=== ë¶„ì„ ë° í”¼ë“œë°± í†µê³„ ë¦¬í¬íŠ¸ ===")
                    report_lines.append(
                        f"ìƒì„± ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
                    )
                    report_lines.append("")

                    # ê¸°ë³¸ í†µê³„
                    report_lines.append("ğŸ“Š ê¸°ë³¸ í†µê³„")
                    report_lines.append(f"- ì´ ë¶„ì„ ê±´ìˆ˜: {total_analyses}")
                    report_lines.append(f"- í”¼ë“œë°± ì™„ë£Œ: {feedback_count}")
                    report_lines.append(f"- í”¼ë“œë°± ëŒ€ê¸°: {feedback_pending}")
                    if total_analyses > 0:
                        report_lines.append(
                            f"- í”¼ë“œë°± ì™„ë£Œìœ¨: {(feedback_count/total_analyses)*100:.1f}%"
                        )
                    report_lines.append("")

                    # íŒì • ê²°ê³¼ ë¶„í¬
                    if "judgment" in df.columns:
                        report_lines.append("ğŸ¯ íŒì • ê²°ê³¼ ë¶„í¬")
                        judgment_counts = df["judgment"].value_counts()
                        for judgment, count in judgment_counts.items():
                            percentage = (count / total_analyses) * 100
                            report_lines.append(
                                f"- {judgment}: {count}ê±´ ({percentage:.1f}%)"
                            )
                        report_lines.append("")

                    # ì •í™•ì„± ë¶„ì„ (í”¼ë“œë°±ì´ ìˆëŠ” ê²½ìš°)
                    if feedback_count > 0:
                        report_lines.append("âœ… ì •í™•ì„± ë¶„ì„")
                        accuracy_counts = {
                            "ì •í™•í•¨": 0,
                            "ë¶€ì •í™•í•¨": 0,
                            "ë¶€ë¶„ì ìœ¼ë¡œ ì •í™•í•¨": 0,
                        }

                        for feedback in feedback_df["user_feedback"]:
                            if "ì •í™•í•¨ âœ…" in str(feedback):
                                accuracy_counts["ì •í™•í•¨"] += 1
                            elif "ë¶€ì •í™•í•¨ âŒ" in str(feedback):
                                accuracy_counts["ë¶€ì •í™•í•¨"] += 1
                            elif "ë¶€ë¶„ì ìœ¼ë¡œ ì •í™•í•¨ âš ï¸" in str(feedback):
                                accuracy_counts["ë¶€ë¶„ì ìœ¼ë¡œ ì •í™•í•¨"] += 1

                        total_feedback = sum(accuracy_counts.values())
                        if total_feedback > 0:
                            for accuracy, count in accuracy_counts.items():
                                percentage = (count / total_feedback) * 100
                                report_lines.append(
                                    f"- {accuracy}: {count}ê±´ ({percentage:.1f}%)"
                                )
                        report_lines.append("")

                    # ëª¨ë¸ë³„ ì„±ëŠ¥ (í•´ë‹¹í•˜ëŠ” ê²½ìš°)
                    if "model_used" in df.columns and feedback_count > 0:
                        report_lines.append("ğŸ¤– ëª¨ë¸ë³„ ì„±ëŠ¥")
                        model_stats = {}

                        for idx, row in feedback_df.iterrows():
                            model = row.get("model_used", "Unknown")
                            feedback = str(row.get("user_feedback", ""))

                            if model not in model_stats:
                                model_stats[model] = {"total": 0, "accurate": 0}

                            model_stats[model]["total"] += 1
                            if "ì •í™•í•¨ âœ…" in feedback:
                                model_stats[model]["accurate"] += 1

                        for model, stats in model_stats.items():
                            if stats["total"] > 0:
                                accuracy_rate = (
                                    stats["accurate"] / stats["total"]
                                ) * 100
                                report_lines.append(
                                    f"- {model}: {accuracy_rate:.1f}% ì •í™•ë„ ({stats['accurate']}/{stats['total']})"
                                )
                        report_lines.append("")

                    # ê°œì„ ì‚¬í•­ ìš”ì•½
                    incorrect_count = len(
                        feedback_df[
                            feedback_df["user_feedback"].str.contains(
                                "ë¶€ì •í™•í•¨", na=False
                            )
                        ]
                    )
                    if incorrect_count > 0:
                        report_lines.append("ğŸ’¡ ê°œì„ ì‚¬í•­")
                        report_lines.append(f"- ë¶€ì •í™•í•œ íŒì •: {incorrect_count}ê±´")

                        # ì˜¬ë°”ë¥¸ íŒì • ë¶„í¬
                        if "correct_judgment" in feedback_df.columns:
                            incorrect_df = feedback_df[
                                feedback_df["user_feedback"].str.contains(
                                    "ë¶€ì •í™•í•¨", na=False
                                )
                            ]
                            correct_judgments = incorrect_df[
                                "correct_judgment"
                            ].value_counts()
                            if not correct_judgments.empty:
                                report_lines.append("- ì‚¬ìš©ì ì œê³µ ì˜¬ë°”ë¥¸ íŒì •:")
                                for judgment, count in correct_judgments.items():
                                    if pd.notna(judgment) and judgment != "":
                                        report_lines.append(
                                            f"  * {judgment}: {count}ê±´"
                                        )
                        report_lines.append("")

                    # ê¸°ê°„ë³„ ë¶„ì„ (ìµœê·¼ 7ì¼)
                    if "timestamp" in df.columns:
                        report_lines.append("ğŸ“… ìµœê·¼ 7ì¼ê°„ í™œë™")
                        now = datetime.now()
                        recent_df = df.copy()
                        recent_df["timestamp"] = pd.to_datetime(recent_df["timestamp"])
                        recent_df = recent_df[
                            recent_df["timestamp"] >= (now - timedelta(days=7))
                        ]

                        if not recent_df.empty:
                            report_lines.append(
                                f"- ìµœê·¼ 7ì¼ ë¶„ì„ ê±´ìˆ˜: {len(recent_df)}"
                            )
                            recent_feedback = len(
                                recent_df[
                                    recent_df["user_feedback"].notna()
                                    & (recent_df["user_feedback"] != "")
                                ]
                            )
                            report_lines.append(
                                f"- ìµœê·¼ 7ì¼ í”¼ë“œë°± ê±´ìˆ˜: {recent_feedback}"
                            )
                        else:
                            report_lines.append("- ìµœê·¼ 7ì¼ê°„ í™œë™ ì—†ìŒ")

                    # ë¦¬í¬íŠ¸ ë¬¸ìì—´ ìƒì„±
                    report_content = "\n".join(report_lines)

                    st.download_button(
                        label="ğŸ“„ í†µê³„ ë¦¬í¬íŠ¸ ë‹¤ìš´ë¡œë“œ",
                        data=report_content,
                        file_name=f"feedback_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                        mime="text/plain",
                    )

                except Exception as e:
                    st.error(f"âŒ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {str(e)}")

        # ìë™ ìƒˆë¡œê³ ì¹¨ ì„¤ì •
        st.subheader("âš™ï¸ ì„¤ì •")

        col1, col2 = st.columns(2)

        with col1:
            auto_refresh = st.checkbox(
                "ìë™ ìƒˆë¡œê³ ì¹¨ í™œì„±í™” (30ì´ˆë§ˆë‹¤)",
                value=st.session_state.get("auto_refresh_enabled", False),
                help="ì²´í¬í•˜ë©´ 30ì´ˆë§ˆë‹¤ ìë™ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìƒˆë¡œê³ ì¹¨í•©ë‹ˆë‹¤",
            )
            st.session_state.auto_refresh_enabled = auto_refresh

            if auto_refresh:
                # 30ì´ˆë§ˆë‹¤ ìë™ ìƒˆë¡œê³ ì¹¨
                import time

                current_time = time.time()
                if current_time - st.session_state.last_refresh_time > 30:
                    st.session_state.last_refresh_time = current_time
                    st.session_state.feedback_refresh_trigger += 1
                    st.cache_data.clear()
                    st.rerun()

        with col2:
            # ìºì‹œ ìˆ˜ë™ í´ë¦¬ì–´
            if st.button("ğŸ—‘ï¸ ìºì‹œ í´ë¦¬ì–´", help="ëª¨ë“  ìºì‹œë¥¼ ìˆ˜ë™ìœ¼ë¡œ í´ë¦¬ì–´í•©ë‹ˆë‹¤"):
                st.cache_data.clear()
                st.session_state.cached_df = None
                st.success("âœ… ìºì‹œê°€ í´ë¦¬ì–´ë˜ì—ˆìŠµë‹ˆë‹¤!")

        # ì‹œìŠ¤í…œ ì •ë³´
        with st.expander("â„¹ï¸ ì‹œìŠ¤í…œ ì •ë³´"):
            st.write(f"**íŒŒì¼ ê²½ë¡œ:**")
            st.code(f"ë¡œê·¸ íŒŒì¼: {LOG_CSV_PATH}")
            st.code(f"ì—…ë¡œë“œ í´ë”: {UPLOAD_FOLDER}")

            st.write(f"**í˜„ì¬ ìƒíƒœ:**")
            st.write(f"- ìƒˆë¡œê³ ì¹¨ íŠ¸ë¦¬ê±°: {st.session_state.feedback_refresh_trigger}")
            st.write(
                f"- ìºì‹œ ìƒíƒœ: {'í™œì„±' if st.session_state.cached_df is not None else 'ë¹„í™œì„±'}"
            )
            st.write(
                f"- ìë™ ìƒˆë¡œê³ ì¹¨: {'í™œì„±' if st.session_state.get('auto_refresh_enabled', False) else 'ë¹„í™œì„±'}"
            )

            # íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
            st.write(f"**íŒŒì¼ ìƒíƒœ:**")
            st.write(
                f"- ë¡œê·¸ íŒŒì¼ ì¡´ì¬: {'âœ…' if os.path.exists(LOG_CSV_PATH) else 'âŒ'}"
            )
            st.write(
                f"- ì—…ë¡œë“œ í´ë” ì¡´ì¬: {'âœ…' if os.path.exists(UPLOAD_FOLDER) else 'âŒ'}"
            )

            if os.path.exists(UPLOAD_FOLDER):
                uploaded_files = [
                    f
                    for f in os.listdir(UPLOAD_FOLDER)
                    if os.path.isfile(os.path.join(UPLOAD_FOLDER, f))
                ]
                st.write(f"- ì—…ë¡œë“œëœ íŒŒì¼ ìˆ˜: {len(uploaded_files)}")

    else:
        st.info("ğŸ“ ì•„ì§ ë¶„ì„ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ì´ë¯¸ì§€ë¥¼ ë¶„ì„í•´ë³´ì„¸ìš”!")

        # ë¹ˆ ìƒíƒœì—ì„œë„ ì‹œìŠ¤í…œ ì •ë³´ëŠ” í‘œì‹œ
        with st.expander("â„¹ï¸ ì‹œìŠ¤í…œ ì •ë³´"):
            st.write(f"**íŒŒì¼ ê²½ë¡œ:**")
            st.code(f"ë¡œê·¸ íŒŒì¼: {LOG_CSV_PATH}")
            st.code(f"ì—…ë¡œë“œ í´ë”: {UPLOAD_FOLDER}")

            st.write(f"**íŒŒì¼ ìƒíƒœ:**")
            st.write(
                f"- ë¡œê·¸ íŒŒì¼ ì¡´ì¬: {'âœ…' if os.path.exists(LOG_CSV_PATH) else 'âŒ'}"
            )
            st.write(
                f"- ì—…ë¡œë“œ í´ë” ì¡´ì¬: {'âœ…' if os.path.exists(UPLOAD_FOLDER) else 'âŒ'}"
            )

# í˜ì´ì§€ í•˜ë‹¨ì— ì‚¬ìš©ë²• ì•ˆë‚´
st.markdown("---")
st.markdown(
    """
### ì‚¬ìš©ë²• ì•ˆë‚´

**í”¼ë“œë°± ê´€ë¦¬ ê¸°ëŠ¥:**
1. **í•„í„°ë§**: í”¼ë“œë°± ìƒíƒœ, íŒì • ê²°ê³¼, ê¸°ê°„ë³„ë¡œ ë°ì´í„°ë¥¼ í•„í„°ë§í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
2. **ê°œë³„ í”¼ë“œë°±**: ê° ë¶„ì„ ê²°ê³¼ì— ëŒ€í•´ ì •í™•ì„± í‰ê°€ì™€ ì½”ë©˜íŠ¸ë¥¼ ì…ë ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
3. **ëŒ€ëŸ‰ ì²˜ë¦¬**: í˜„ì¬ í˜ì´ì§€ì˜ ëª¨ë“  í•­ëª©ì— ëŒ€í•´ ì¼ê´„ ì²˜ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤
4. **ë°ì´í„° ê´€ë¦¬**: ê°œë³„ ì‚­ì œ, ëŒ€ëŸ‰ ì‚­ì œ, í”¼ë“œë°± ì´ˆê¸°í™” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤ 
+ í”¼ë“œë°± ê´€ë¦¬ ì°½ì—ì„œ ë°ì´í„° ì‚­ì œëŠ” ì „ë¶€ ì‚­ì œê³ , ë°ì´í„° ë¶„ì„ ì°½ ë°ì´í„° ì‚­ì œëŠ” ê·¸ íƒ­ì˜ ë°ì´í„°ë§Œ ì‚­ì œë©ë‹ˆë‹¤.

**í†µê³„ ë° ë¶„ì„:**
- í”¼ë“œë°± í˜„í™©ê³¼ ì •í™•ë„ë¥¼ ì‹œê°ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ëª¨ë¸ë³„ ì„±ëŠ¥ ë¹„êµê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤
- ê°œì„ ì‚¬í•­ì„ ìë™ìœ¼ë¡œ ìš”ì•½í•´ì¤ë‹ˆë‹¤

**ë°ì´í„° ë‚´ë³´ë‚´ê¸°:**
- ì „ì²´ ë°ì´í„° ë˜ëŠ” í”¼ë“œë°± ë°ì´í„°ë§Œ CSVë¡œ ë‹¤ìš´ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤
- ìƒì„¸í•œ í†µê³„ ë¦¬í¬íŠ¸ë¥¼ í…ìŠ¤íŠ¸ íŒŒì¼ë¡œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤

ğŸ’¡ **íŒ**: ìë™ ìƒˆë¡œê³ ì¹¨ì„ í™œì„±í™”í•˜ë©´ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°ì´í„° ë³€ê²½ì‚¬í•­ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""
)
```
